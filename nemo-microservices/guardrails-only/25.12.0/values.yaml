# For implementation of ezua values, go to virtualService below
ezua:
  #Use next options in order to configure the application endpoint.
  virtualService:
    endpoint: "nemo-microservices.${DOMAIN_NAME}/docs"
    istioGateway: "istio-system/ezaf-gateway"

tags:
  # -- When set to `true`, installs the primary NeMo microservices platform. To install individual microservices, set `platform` to `false` and configure the `tags` key with values for each individual microservice. For more information, refer to [Tag-Based Installation](https://docs.nvidia.com/nemo/microservices/latest/set-up/deployment-options.html#tag-based-installation).
  platform: false
  # -- Specifies whether to install the NeMo Guardrails microservice.
  guardrails: true
  # -- Specifies whether to install the NeMo Customizer microservice.
  customizer: false
  # -- Specifies whether to install the NeMo Data Designer microservice.
  data-designer: false
  # -- Specifies whether to install the NeMo Evaluator microservice.
  evaluator: false
  # -- Specifies whether to install the NeMo Auditor microservice.
  auditor: false
  # -- Specifies whether to install the NeMo Studio UI microservice.
  studio: false
  # -- Specifies whether to install the NeMo Safe Synthesizer microservice.
  safe-synthesizer: false
  # -- Specifies whether to install the NeMo Core microservice.
  core: false
  # -- Specifies whether to install the NeMo Intake microservice.
  intake: false

# -- You can use an existing Kubernetes secret for communicating with the NGC API for downloading models. The chart uses the `ngcAPIKey` value to generate the secret if you set this to an empty string.
existingSecret: ""

# -- You can specify an existing Kubernetes image pull secret for pulling images from the NGC container registry. The chart uses the `ngcAPIKey` value to generate the secret if you set this to an empty string.
existingImagePullSecret: ""

# -- Your NVIDIA GPU Cloud (NGC) API key authenticates and enables pulling images from the NGC container registry. The existing secret overrides this key if you provide one to the `existingSecret` key.
ngcAPIKey: YOUR-NGC-API-KEY

# -- List of image pull secrets. Existing secrets override these values if you specify them. Use this only for experimentation when you want to hardcode a secret in your values file.
# @default -- `[{"name":"nvcrimagepullsecret","password":"YOUR-NGC-API-KEY","registry":"nvcr.io","username":"$$oauthtoken"}]`
imagePullSecrets:
  - name: "{{ (index .Values.global.imagePullSecrets 0).name }}"
    registry: nvcr.io
    username: $oauthtoken
    password: "{{ .Values.ngcAPIKey }}"

# Whether to add AIE self-signed certificates to root CAs
# Requires a trust-manager bundle with useDefaultCAs set to true
# More details here https://gist.github.hpe.com/jeff-oxenberg/70462e0777bf02c38be9823324de9f6d
certificatesPolicy:
  enabled: false
  # The list of environment variables to inject
  env:
    - name: SSL_CERT_DIR
      value: /etc/ssl/certs
    - name: REQUESTS_CA_BUNDLE
      value: /etc/ssl/certs/ca-certificates.crt
    - name: SSL_CERT_FILE
      value: /etc/ssl/certs/ca-certificates.crt
    - name: AIOHTTP_CLIENT_SESSION_SSL
      value: /etc/ssl/certs/ca-certificates.crt
  # The list of volume mounts for the containers
  volumeMounts:
    - mountPath: /etc/ssl/certs/ca-certificates.crt
      name: etc-ssl-certs
      readOnly: true
      subPath: ca-certificates.crt
  # The volume definition (at the pod level)
  volumes:
    - name: etc-ssl-certs
      configMap:
        name: ezaf-root-ca
        items:
          - key: ezaf-root-ca.crt
            path: ca-certificates.crt

global:
  # -- The name of the image pull secret to use globally across all services.
  imagePullSecrets:
    - name: nvcrimagepullsecret
  # -- A value needed for bitnamilegacy images used in postgresql subcharts
  security:
    allowInsecureImages: true

  # -- The name of the config map to use for global platform configuration.
  platformConfigmapName: nemo-platform-config

data-store:
  # -- Global parameters to override the same settings in all subcharts of the data-store Helm chart.
  global:
    # -- Global image registry.
    imageRegistry: ""
    ## E.g.
    ## imagePullSecrets:
    ##   - myRegistryKeySecretName
    ##
    # -- Global image pull secrets.
    imagePullSecrets: []
    # -- Global storage class that applies to persistent volumes.
    storageClass: ""
    # -- Global host aliases which will be added to the pod's hosts files.
    hostAliases: []
    # - ip: 192.168.137.2
    #   hostnames:
    #   - example.com

  # -- The service name for the NeMo Data Store microservice.
  serviceName: nemo-data-store

  # -- Number of replicas for the deployment.
  replicaCount: 1

  # -- Strategy configuration for controlling how pod updates are performed.
  # @default -- This object has the following default values for the strategy configuration.
  strategy:
    # -- The strategy type for pod updates. Use `RollingUpdate` if you use `ReadWriteMany` persistent storage or deploying on a single node. Otherwise, use `Recreate` to allow upgrades, although it causes downtime during the upgrade process.
    type: "Recreate"
    rollingUpdate:
      # -- Maximum number of pods that can be created above the desired amount during an update.
      maxSurge: "100%"
      # -- Maximum number of pods that can be unavailable during an update.
      maxUnavailable: 0

  # -- Kubernetes cluster domain name.
  clusterDomain: cluster.local

  # -- Container image configuration settings
  # @default -- This object has the following default values for the image configuration.
  image:
    # -- The registry where the NeMo Data Store image is located.
    registry: "nvcr.io"
    # -- The repository path of the NeMo Data Store image.
    repository: "nvidia/nemo-microservices/datastore"
    # -- The image tag to use.
    tag: ""
    # -- The image digest to use for more precise version control.
    digest: ""
    # -- The image pull policy determining when to pull new images.
    pullPolicy: IfNotPresent
    # -- Whether to run the container with rootless security context.
    rootless: true
    # -- Complete override string for the image specification.
    fullOverride: ""

  # -- Configuration for image pull secrets to access private registries.
  imagePullSecrets:
    # -- Name of the secret containing registry credentials.
    - name: nvcrimagepullsecret

  # -- Pod-level security context settings
  podSecurityContext:
    # -- The file system group ID to use for all containers.
    fsGroup: 1000
    # -- Set the permission change policy for mounted PVCs.
    fsGroupChangePolicy: OnRootMismatch

  # -- Container-level security context settings
  containerSecurityContext: {}
  #   allowPrivilegeEscalation: false
  #   capabilities:
  #     drop:
  #       - ALL
  #   # Add the SYS_CHROOT capability for root and rootless images if you intend to
  #   # run pods on nodes that use the container runtime cri-o. Otherwise, you will
  #   # get an error message from the SSH server that it is not possible to read from
  #   # the repository.
  #     add:
  #       - SYS_CHROOT
  #   privileged: false
  #   readOnlyRootFilesystem: true
  #   runAsGroup: 1000
  #   runAsNonRoot: true
  #   runAsUser: 1000

  # -- **DEPRECATED** Run init and NeMo Data Store containers as a specific securityContext. The securityContext variable has been split two: `containerSecurityContext` and `podSecurityContext`.
  securityContext: {}

  # -- Pod disruption budget configuration.
  podDisruptionBudget: {}
  #  maxUnavailable: 1
  #  minAvailable: 1

  # -- Service configuration for exposing the application.
  # @default -- This object has the following default values for the service configuration.
  service:
    # -- HTTP service configuration.
    http:
      # -- The Kubernetes service type to create for HTTP traffic.
      type: ClusterIP
      # -- The port number to expose for HTTP traffic.
      port: 3000
      # -- The cluster IP address to assign to the service.
      clusterIP: ""
      # -- The static IP address for LoadBalancer service type.
      loadBalancerIP:
      # -- The node port number when using NodePort service type
      nodePort:
      # -- External traffic policy for controlling source IP preservation
      externalTrafficPolicy:
      # -- List of external IP addresses to assign to the service
      externalIPs:
      # -- IP family policy for dual-stack support
      ipFamilyPolicy:
      # -- List of IP families to use for the service
      ipFamilies:
      # -- List of CIDR ranges allowed to access the LoadBalancer
      loadBalancerSourceRanges: []
      # -- Additional annotations for the HTTP service
      annotations: {}
      # -- Additional labels for the HTTP service
      labels: {}
    # -- SSH service configuration.
    # @default -- This object has the following default values for the SSH service configuration.
    ssh:
      # -- Whether to enable SSH service.
      enabled: false
      # -- The Kubernetes service type to create for SSH traffic.
      type: ClusterIP
      # -- The port number to expose for SSH traffic.
      port: 22
      # -- The cluster IP address to assign to the service.
      clusterIP: None
      # -- The static IP address for LoadBalancer service type.
      loadBalancerIP:
      # -- The node port number when using NodePort service type.
      nodePort:
      # -- The external traffic policy for controlling source IP preservation.
      externalTrafficPolicy:
      # -- List of external IP addresses to assign to the service.
      externalIPs:
      # -- IP family policy for dual-stack support.
      ipFamilyPolicy:
      # -- List of IP families to use for the service.
      ipFamilies:
      # -- The host port number when using HostPort service type.
      hostPort:
      # -- List of CIDR ranges allowed to access the LoadBalancer.
      loadBalancerSourceRanges: []
      # -- Additional annotations for the SSH service.
      annotations: {}
      # -- Additional labels for the SSH service.
      labels: {}

  # -- Kubernetes deployment resources configuration.
  # It is recommended to not specify default resources and to leave this as a conscious
  # choice. This also increases chances that the chart will run on environments with little
  # resources, such as minikube. If you want to specify resources, use the following
  # example, adjust the values as necessary, and remove the empty curly braces `{}`.
  # `limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi`
  resources: {}

  # -- The name of the alternate scheduler to use. For more information, see [Configure Multiple Schedulers](https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/).
  schedulerName: ""

  # -- NodeSelector configuration for the deployment.
  nodeSelector: {}

  # -- Tolerations configuration for the deployment.
  tolerations: []

  # -- Affinity configuration for the deployment.
  affinity: {}

  # -- TopologySpreadConstraints configuration for the deployment.
  topologySpreadConstraints: []

  # -- dnsConfig configuration for the deployment.
  dnsConfig: {}

  # -- priorityClassName configuration for the deployment.
  priorityClassName: ""

  # -- Deployment configuration.
  deployment:
    # -- How long to wait until forcefully kill the pod.
    terminationGracePeriodSeconds: 60
    # -- Labels for the deployment.
    labels: {}
    # -- Annotations for the Datastore deployment to be created.
    annotations: {}

  # -- Additional environment variables to pass to containers. This is an object formatted like NAME: value or NAME: valueFrom: {object}.
  env: {}

  # -- Service account configuration.
  serviceAccount:
    # -- Whether to create a service account.
    create: true
    # -- Name of the created service account, defaults to release name. Can also link to an externally provided service account that should be used.
    name: "gitea"
    # -- Enable/disable auto mounting of the service account token.
    automountServiceAccountToken: false
    # -- Image pull secrets, available to the service account. To add a list of image pull secrets, remove `[]`use the following format: - name: private-registry-access.
    imagePullSecrets: []
    # -- Custom annotations for the service account.
    annotations: {}
    # -- Custom labels for the service account.
    labels: {}

  # -- Persistence volume configuration.
  # @default -- This object has the following default values for the persistence volume configuration.
  persistence:
    # -- Whether to enable persistent volume.
    enabled: true
    # -- Whether to create the persistent volume claim for shared storage.
    create: true
    # -- Whether to mount the persistent volume claim.
    mount: true
    # -- Name of the persistent volume claim. You can use an existing claim to store repository information.
    claimName: datastore-shared-storage
    # -- The size of the persistent volume.
    size: 100Gi
    # -- Access modes for the persistent volume.
    accessModes:
      - ReadWriteOnce
    # -- Labels for the persistence volume claim.
    labels: {}
    # -- Name of the storage class to use.
    storageClass:
    # -- Subdirectory of the volume to mount at.
    subPath:
    # -- Name of persistent volume in PVC.
    volumeName: ""
    # -- Annotations for the persistence volume claim.
    annotations:
      helm.sh/resource-policy: keep

  # -- Additional volumes to mount to the Datastore deployment.
  extraVolumes: []
  # - name: postgres-ssl-vol
  #   secret:
  #     secretName: Datastore-postgres-ssl

  # -- Mounts that are only mapped into the Datastore runtime/main container, to e.g. override custom templates.
  extraContainerVolumeMounts: []

  # -- Mounts that are only mapped into the init-containers. Can be used for additional preconfiguration.
  extraInitVolumeMounts: []

  # -- **DEPRECATED** Additional volume mounts for init containers and the Datastore main container. This value is split into the following two variables: `extraContainerVolumeMounts` and `extraInitVolumeMounts`.
  extraVolumeMounts: []
  # - name: postgres-ssl-vol
  #   readOnly: true
  #   mountPath: "/pg-ssl"

  # -- Init container Bash shell scripts. For example, to mount a client certificate when connecting to an external Postgres server, you might add commands similar to the following:
  #   `mkdir -p /data/git/.postgresql`,
  #   `cp /pg-ssl/* /data/git/.postgresql/`,
  #   `chown -R git:git /data/git/.postgresql/`,
  #   `chmod 400 /data/git/.postgresql/postgresql.key`
  initPreScript: ""

  # -- Kubernetes resource limits for init containers.
  initContainers:
    resources:
      # -- Kubernetes resource limits for init containers.
      limits: {}
      # -- Kubernetes cpu resource limits for init containers.
      requests:
        cpu: 100m
        memory: 128Mi

  # -- Signing configuration.
  signing:
    # -- Enable commit/action signing.
    enabled: false
    # -- GPG home directory.
    gpgHome: /data/git/.gnupg
    # -- Inline private GPG key for signed NeMo Data Store actions.
    privateKey: ""
    # privateKey: |-
    #   -----BEGIN PGP PRIVATE KEY BLOCK-----
    #   ...
    #   -----END PGP PRIVATE KEY BLOCK-----
    # -- Use an existing secret to store the value of `signing.privateKey`.
    existingSecret: ""

  # -- Admin user configuration settings.
  admin:
    # -- Username for the NeMo Data Store admin user.
    username: datastore_admin
    # -- Use an existing secret to store admin user credentials. For example, `datastore-admin-secret`.
    existingSecret:
    # -- Password for the NeMo Data Store admin user.
    password: s3aJPHD9!bt6d0I
    # -- Email for the NeMo Data Store admin user.
    email: "datastore@local.domain"

  metrics:
    # -- Enable NeMo Data Store metrics. Also requires setting env variable GITEA__metrics__ENABLED: "true"
    enabled: false
    serviceMonitor:
      # -- Enable NeMo Data Store metrics service monitor.
      enabled: false

  # -- LDAP configuration.
  ldap:
    []
    # - name: "LDAP 1"
    #  existingSecret:
    #  securityProtocol:
    #  host:
    #  port:
    #  userSearchBase:
    #  userFilter:
    #  adminFilter:
    #  emailAttribute:
    #  bindDn:
    #  bindPassword:
    #  usernameAttribute:
    #  publicSSHKeyAttribute:

  # Either specify inline `key` and `secret` or refer to them via `existingSecret`
  # -- OAuth configuration.
  oauth:
    []
    # - name: 'OAuth 1'
    #   provider:
    #   key:
    #   secret:
    #   existingSecret:
    #   autoDiscoverUrl:
    #   useCustomUrls:
    #   customAuthUrl:
    #   customTokenUrl:
    #   customProfileUrl:
    #   customEmailUrl:

  # -- NeMo Data Store configuration.
  # @default -- This object has the following default values for the NeMo Data Store configuration.
  config:
    # -- Application name.
    APP_NAME: "Datastore"
    # -- Runtime mode (prod/dev).
    RUN_MODE: prod
    server:
      # -- HTTP port for web interface.
      HTTP_PORT: 3000
      # -- Enable SSH server.
      START_SSH_SERVER: false
      # -- Enable LFS server.
      LFS_START_SERVER: true
    cron.GIT_GC_REPOS:
      # -- Enable git garbage collection.
      enabled: false
    lfs:
      # -- Storage type for LFS (local/s3).
      STORAGE_TYPE: local
    session:
      # -- Session provider type.
      PROVIDER: memory
    cache:
      # -- Cache adapter type.
      ADAPTER: memory
    queue:
      # -- Queue type.
      TYPE: dummy
    database:
      # -- Database type.
      DB_TYPE: postgres

  # -- Additional configuration from secret or configmap.
  additionalConfigSources: []
  #   - secret:
  #       secretName: Datastore-app-ini-oauth
  #   - configMap:
  #       name: Datastore-app-ini-plaintext

  # -- Additional configuration sources from environment variables.
  additionalConfigFromEnvs: []

  # -- Annotations for the Datastore pod.
  podAnnotations: {}

  # -- Configure OpenSSH's log level. Only available for root-based Datastore image.
  ssh:
    logLevel: "INFO"

  # -- Liveness probe configuration.
  # @default -- This object has the following default values for the liveness probe configuration.
  livenessProbe:
    # -- Enable liveness probe.
    enabled: true
    httpGet:
      # -- HTTP path for liveness probe.
      path: /v1/health
      # -- Port for liveness probe.
      port: http
    # -- Initial delay before liveness probe is initiated.
    initialDelaySeconds: 10
    # -- Timeout for liveness probe.
    timeoutSeconds: 5
    # -- Period for liveness probe.
    periodSeconds: 30
    # -- Success threshold for liveness probe.
    successThreshold: 1
    # -- Failure threshold for liveness probe.
    failureThreshold: 20

  # -- Readiness probe configuration.
  # @default -- This object has the following default values for the readiness probe configuration.
  readinessProbe:
    # -- Enable readiness probe.
    enabled: true
    httpGet:
      # -- HTTP path for readiness probe.
      path: /v1/health
      # -- Port for readiness probe.
      port: http
    # -- Initial delay before readiness probe is initiated.
    initialDelaySeconds: 30
    # -- Timeout for readiness probe.
    timeoutSeconds: 30
    # -- Period for readiness probe.
    periodSeconds: 20
    # -- Success threshold for readiness probe.
    successThreshold: 1
    # -- Failure threshold for readiness probe.
    failureThreshold: 40

  # -- Start-up probe configuration.
  # @default -- This object has the following default values for the start-up probe configuration.
  startupProbe:
    # -- Enable start-up probe.
    enabled: false
    # -- TCP socket configuration for start-up probe.
    tcpSocket:
      port: http
    # -- Initial delay before start-up probe is initiated.
    initialDelaySeconds: 60
    # -- Timeout for start-up probe.
    timeoutSeconds: 1
    # -- Period for start-up probe.
    periodSeconds: 10
    # -- Success threshold for start-up probe.
    successThreshold: 1
    # -- Failure threshold for start-up probe.
    failureThreshold: 10

  # -- Redis cluster configuration.
  # @default -- This object has the following default values for the Redis cluster configuration.
  redis-cluster:
    # -- Enable Redis cluster.
    enabled: false
    # -- Whether to use password authentication.
    usePassword: false
    # -- Number of redis cluster master nodes.
    cluster:
      nodes: 3 # default: 6
      replicas: 0 # default: 1

  # -- PostgreSQL high availability (HA) configuration.
  # @default -- This object has the following default values for the PostgreSQL HA configuration.
  postgresql-ha:
    global:
      postgresql:
        # -- Global PostgreSQL database name.
        database: datastore
        # -- Global password for the `datastore` user.
        password: datastore
        # -- Global username for the `datastore` user.
        username: datastore
    # -- Enable PostgreSQL HA. If enabled, configures PostgreSQL HA using the [bitnami/postgresql-ha](https://github.com/bitnami/charts/tree/main/bitnami/postgresql-ha) chart.
    enabled: false
    volumePermissions:
      image:
        repository: bitnamilegacy/os-shell
    metrics:
      image:
        repository: bitnamilegacy/postgres-exporter
    postgresql:
      image:
        repository: bitnamilegacy/postgresql
      # -- Repmgr password for the `datastore` user.
      repmgrPassword: changeme2
      # -- Postgres password for the `datastore` user.
      postgresPassword: changeme1
      # -- Password for the `datastore` user.
      password: changeme4
    pgpool:
      # -- Pgpool admin password.
      adminPassword: changeme3
    service:
      # -- PostgreSQL service port.
      ports:
        postgresql: 5432
    primary:
      # -- PVC storage request for PostgreSQL HA volume.
      persistence:
        size: 10Gi

  postgresql:
    # -- Enable or disable the built-in PostgreSQL database.
    enabled: true
    image:
      repository: bitnamilegacy/postgresql
    volumePermissions:
      image:
        repository: bitnamilegacy/os-shell
    metrics:
      image:
        repository: bitnamilegacy/postgres-exporter
    global:
      postgresql:
        auth:
          # -- Password for the datastore database user.
          password: datastore
          # -- Name of the database to create.
          database: datastore
          # -- Username for the database user.
          username: datastore
        service:
          ports:
            # -- Port number for PostgreSQL service.
            postgresql: 5432
    primary:
      persistence:
        # -- Storage size request for the PostgreSQL persistent volume.
        size: 10Gi

  test:
    # -- Enable or disable the test-connection Pod.
    enabled: false
    image:
      # -- Image name for the wget container used in the test-connection Pod.
      name: busybox
      # -- Image tag for the wget container used in the test-connection Pod.
      tag: latest

  # -- Set to false to skip the basic validation check.
  checkDeprecation: true

  # -- Array of extra objects to deploy with the release.
  extraDeploy: []

  # -- Whether to serve traffic directly from an object storage service.
  serveDirect: true

  # -- Object Store configuration settings for accessing external Object Storage.
  # @default -- This object has the following default values for the object store configuration.
  objectStore:
    # -- Enable or disable object storage integration.
    enabled: false
    # -- Object storage service endpoint URL.
    endpoint: ""
    # -- Access key credential for object storage authentication.
    accessKey: ""
    # -- Secret key credential for object storage authentication.
    accessSecret: ""
    # -- Name of the bucket to use for object storage.
    bucketName: "datastore"
    # -- Geographic region for the object storage service.
    region: ""
    # -- Enable or disable SSL/TLS for object storage connections.
    ssl: false
    # -- Name of existing Kubernetes secret containing object storage credentials.
    existingSecret: ""
    # -- Key in existing secret that contains the access key.
    existingSecretAccessKey: ""
    # -- Key in existing secret that contains the secret key.
    existingSecretAccessSecret: ""

  # -- External URL configuration for the NeMo Data Store microservice.
  external:
    # -- The external URL where users will access the NeMo Data Store microservice.
    rootUrl: http://data-store.test
    # -- The external URL's domain name.
    domain: data-store.test

  # -- External PostgreSQL configuration settings. These values are only used when postgresql.enabled is set to false.
  # @default -- This object has the following default values for the external PostgreSQL configuration.
  externalDatabase:
    # -- External database host address.
    host: ""
    # -- External database port number.
    port: 5432
    # -- Database username for Datastore service.
    user: ""
    # -- Datastore database name.
    database: ""
    # -- Name of an existing secret resource containing the database credentials.
    existingSecret: ""
    # -- Name of an existing secret key containing the database credentials.
    existingSecretPasswordKey: ""
    # -- SSL mode for external database connection.
    sslMode: "disable"

  # -- JWT secret configuration settings.
  # @default -- This object has the following default values for the JWT secret configuration.
  jwtSecret:
    # -- User specified LFS JWT secret - this will be stored in a secret.
    value: ""
    # -- Name of an existing secret resource containing the LFS JWT secret.
    existingSecret: ""
    # -- Key in existing secret containing the LFS JWT secret.
    existingSecretKey: ""

  # -- Parameters for the demo mode.
  # @default -- This object has the following default values for the demo parameters.
  demo:
    # -- Enable or disable the demo mode.
    enabled: false
    # -- Name of the secret containing the NGC image pull secret.
    ngcImagePullSecret: ngc-image-pull-secret
    # -- NGC API key.
    ngcApiKey: ""

customizer:
  # -- String to override chart name on resulting objects when deployed.
  nameOverride: ""
  # -- String to fully override the chart and release name on resulting objects when deployed.
  fullnameOverride: ""

  # -- NeMo Customizer image that supports training and standalone mode.
  # @default -- This object has the following default values for the NeMo Customizer microservice image.
  image:
    # -- Registry for the NeMo Customizer image.
    registry: nvcr.io
    # -- Repository for the NeMo Customizer image.
    repository: nvidia/nemo-microservices/customizer
    # -- Image pull policy for the NeMo Customizer image.
    imagePullPolicy: IfNotPresent

  # -- NeMo Customizer image that supports training for HuggingFace models using NeMo AutoModel
  # @default -- This object has the following default values for the NeMo Customizer AutoModel microservice image.
  automodelImage:
    # -- Registry for the NeMo Customizer AutoModel image.
    registry: nvcr.io
    # -- Repository for the NeMo Customizer AutoModel image.
    repository: nvidia/nemo-microservices/customizer-automodel
    # -- Image pull policy for the NeMo Customizer AutoModel image.
    imagePullPolicy: IfNotPresent

  # -- NeMo Customizer RL image that provides alignment training functionality for the NeMo Customizer ecosystem.
  # @default -- This object has the following default values for the NeMo Customizer RL image.
  nemoRlImage:
    # -- Registry for the NeMo Customizer training image used for alignment.
    registry: nvcr.io
    # -- Repository for the NeMo Customizer training image used for alignment.
    repository: nvidia/nemo-microservices/customizer-rl
    # -- Image pull policy for the NeMo Customizer training image used for alignment.
    imagePullPolicy: IfNotPresent

  # -- Customizer API only image configuration.
  # @default -- This object has the following default values for the NeMo Customizer API only image.
  apiImage:
    # -- Registry for the NeMo Customizer API image.
    registry: nvcr.io
    # -- Repository for the NeMo Customizer API image.
    repository: nvidia/nemo-microservices/customizer-api
    # -- Image pull policy for the NeMo Customizer API image.
    imagePullPolicy: IfNotPresent

  # -- Image pull secrets configuration.
  imagePullSecrets:
    - name: nvcrimagepullsecret

  # -- Download models to PVC model cache configuration.
  # @default -- This object has the following default values for the model downloader.
  modelDownloader:
    # -- Security context for the model downloader.
    securityContext:
      fsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
      runAsGroup: 1000
    # -- Time to live in seconds after the job finishes.
    ttlSecondsAfterFinished: 1800

  # -- Secret used for auto hydrating the model cache from NGC for enabled models.
  ngcAPISecret: "ngc-api"
  # -- Key in the NGC API secret containing the API key.
  ngcAPISecretKey: "NGC_API_KEY"
  # -- The K8s Secret containing the HuggingFace API token.
  hfAPISecret: null
  # -- The key in the hfAPISecret containing the actual secret's value. Defaults to HF_TOKEN
  hfAPISecretKey: "HF_TOKEN"

  # -- A map of environment variables to inject into the NeMo Customizer app container.
  # Example:
  #
  # `{HOST_IP:
  #   valueFrom:
  #     fieldRef:
  #       fieldPath: status.hostIP
  # OTEL_EXPORTER_OTLP_ENDPOINT: "http://$(HOST_IP):4317"}`
  env:
    # How long to wait until automatically deleting the NemoTrainingJob
    JOB_CLEANUP_TTL_SEC: "3600"
    # How often in seconds to poll for status conditions from NemoTrainingJob
    JOB_STATUS_POLLING_INTERVAL: "15"
    # How often in seconds to poll for events and logs for all objects owned by NemoTrainingJob, including
    # volcano/runai jobs, pods. This is longer because it is a more expensive IO operation.
    JOB_LOGS_POLLING_INTERVAL: "600"
    # Given a failure in the watch loop, how long to sleep in secs before iterating a new loop.
    TRAINING_JOB_CREATION_FAILURE_SLEEP_INTERVAL: "15"
    # How many failures to tolerate in the NemoTrainingJob watcher loop before terminating the loop and the job itself.
    MAX_TRAINING_JOB_STATUS_FAILURES: "10"
    # How many minutes to wait for a GPU or Node to become available before automatically cancelling a job
    MAX_PENDING_JOB_MINUTES: "30"

  readinessProbe:
    initialDelaySeconds: 30
    timeoutSeconds: 15
    failureThreshold: 15

  livenessProbe:
    initialDelaySeconds: 30
    timeoutSeconds: 15
    failureThreshold: 15

  # -- Tools configuration for downloading and uploading entities to NeMo Data Store.
  # @default -- This object has the following default values for the NeMo Data Store tools image.
  nemoDataStoreTools:
    # -- Registry for the NeMo Data Store tools image.
    registry: nvcr.io
    # -- Repository for the NeMo Data Store tools image.
    repository: nvidia/nemo-microservices/nds-v2-huggingface-cli
    # -- Tag for the NeMo Data Store tools image.
    tag: ""
    # -- Image pull secret for the NeMo Data Store tools image.
    imagePullSecret: nvcrimagepullsecret

  # -- Service configuration.
  service:
    # -- Type of Kubernetes service to create.
    type: ClusterIP
    # -- External port for the service.
    port: 8000
    # -- Internal port for the service.
    internalPort: 9009

  # -- Number of replicas to deploy.
  replicaCount: 1

  # -- Service account configuration.
  serviceAccount:
    # -- Specifies whether a service account should be created.
    create: true
    # -- The name of the service account to use. If not set and create is true, a name is generated.
    name: ""
    # -- Annotations to add to the service account.
    annotations: {}
    # -- Automatically mount a ServiceAccount's API credentials.
    automountServiceAccountToken: true

  # -- Configure the PVC for models mount, where we store the parent/base models.
  modelsStorage:
    # -- Enable persistent volume for model storage.
    enabled: true
    # -- Storage class name for the models PVC. Empty string uses the default storage class.
    storageClassName: ""
    # -- Size of the persistent volume.
    size: 100Gi
    # -- Access modes for the persistent volume.
    accessModes:
      - ReadWriteOnce

  # -- Logging configuration.
  logging:
    # -- Log level for the application.
    logLevel: INFO
    # -- Enable logging for health endpoints.
    logHealthEndpoints: false

  # -- Configuration for the NeMo Customizer microservice.
  # @default -- This object has default values for the following fields.
  customizerConfig:
    # -- Specifies the internal K8s DNS record for the NeMo Data Store service.
    nemoDataStoreURL: "http://nemo-data-store:3000"
    # -- Specifies the internal K8s DNS record for the NeMo Entity Store service.
    entityStoreURL: "http://nemo-entity-store:8000"
    # -- URL for the MLflow tracking server.
    mlflowURL: "" # "http://mlflow-tracking.mlflow-system.svc.cluster.local:80"

    # -- Weights and Biases (WandB) Python SDK intialization configuration for logging and monitoring training jobs in WandB.
    wandb:
      # -- The username or team name under which the runs will be logged.
      # -- If not specified, the run will default to a default entity set in the account settings.
      # -- To change the default entity, go to the account settings https://wandb.ai/settings
      # -- and update the "Default location to create new projects" under "Default team".
      # -- Reference: https://docs.wandb.ai/ref/python/init/
      entity: null
      # The name of the project under which this run will be logged
      project: "nvidia-nemo-customizer"

    # -- Network configuration for training jobs on Oracle Kubernetes Engine (OKE) on Oracle Cloud Infrastructure (OCI).
    trainingNetworking:
      - name: NCCL_IB_SL
        value: 0
      - name: NCCL_IB_TC
        value: 41
      - name: NCCL_IB_QPS_PER_CONNECTION
        value: 4
      - name: UCX_TLS
        value: TCP
      - name: UCX_NET_DEVICES
        value: eth0
      - name: HCOLL_ENABLE_MCAST_ALL
        value: 0
      - name: NCCL_IB_GID_INDEX
        value: 3

    # -- Training configuration for customization jobs.
    # @default -- This object has the following default values for the training configuration.
    training:
      # -- Queue name used by the underlying scheduler of NemoTrainingJob.
      # Maps to "resourceGroup" in NemoTrainingJob.
      queue: "default"

      # -- Directory path for training workspace.
      workspace_dir: "/pvc/workspace"

      # -- Training timeout in seconds. If job times out, it will be marked as failed and no checkpoints are saved.
      # training_timeout: 3600

      # -- Interval in seconds to poll for monitoring jobs.
      # Defaults to 10s. poll_interval_seconds with a 30 second pad must be less than ttl_seconds_after_finished.
      poll_interval_seconds: 10

      # -- Time to live in seconds after the training job pod completes.
      # Defaults to 1h. Take precautions when setting ttl_seconds_after_finished to 0 which disables automatic clean up for
      # jobs. When disabled, jobs will persist and hold on to resources like PVCs and will require manual or external clean up.
      # ttl_seconds_after_finished must be greater than poll_interval_seconds with a 30 second pad.
      ttl_seconds_after_finished: 3600

      # container_defaults lets you configure the training container similar to a K8s object.
      # Currently, it only supports a subset of the K8s Container Spec.
      # Note it is named _defaults because we plan to allow training specific sections in the future that can override this section.
      # -- Default container configuration for training jobs.
      container_defaults:
        # env holds a list of custom environment variables injected into the training container.
        # However, they cannot override env variables reserved by Customizer. The application validates this at start time.
        # An example use case is configuring OpenTelemetry in the training container.
        # By default, the training container inherits its OpenTelemetry environment values from the openTelemetry section.
        # However, by setting the OpenTelemetry env variables here, the user can override the behavior set in the openTelemetry section.
        # -- Environment variables for the training container.
        # Cannot override env variables reserved by NeMo Customizer.
        env:
          # - name: HOST_IP
          #   valueFrom:
          #     fieldRef:
          #       fieldPath: "status.hostIP"
          # - name: NAMESPACE
          #   valueFrom:
          #     fieldRef:
          #       fieldPath: "metadata.namespace"
          # - name: OTEL_RESOURCE_ATTRIBUTES
          #   value: "deployment.environment=$(NAMESPACE)"
          # - name: OTEL_EXPORTER_OTLP_ENDPOINT
          #   value: "http://$(HOST_IP):4317"
          # - name: OTEL_TRACES_EXPORTER
          #   value: otlp
          # - name: OTEL_METRICS_EXPORTER
          #   value: otlp
          # - name: OTEL_LOGS_EXPORTER
          #   value: none
        imagePullPolicy: IfNotPresent
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
          runAsGroup: 1000
      # PVC config for NemoTrainingJob, which automatically creates one for each job
      # This is implicitly mounted at /pvc to our training container
      pvc:
        # -- Storage class for the training job PVC.
        storageClass: ""
        # -- Size of the training job PVC.
        size: 5Gi
        # -- Volume access mode for the training job PVC.
        volumeAccessMode: "ReadWriteOnce"
        # -- The name of a single PVC to be used for training. If null, create a separate PVC per training job for isolation. If provided, it will create this PVC.
        name: null

    # -- OpenTelemetry settings.
    # @default -- This object has the following default values for the OpenTelemetry settings.
    openTelemetry:
      # -- Whether to enable OpenTelemetry.
      enabled: true
      # -- Sets the traces exporter type (otlp, console, none).
      tracesExporter: otlp
      # -- Sets the metrics exporter type (otlp, console, none).
      metricsExporter: otlp
      # -- Sets the logs exporter type (otlp, console, none).
      logsExporter: otlp
      # -- Endpoint to access a custom OTLP collector listening on port 4317. Example: "http://$(HOST_IP):4317".
      exporterOtlpEndpoint: ""

    # -- Tolerations on the customization job pods.
    tolerations: []

  # -- PostgreSQL configuration for the NeMo Customizer microservice.
  # @default -- This object has the following default values for the PostgreSQL configuration.
  postgresql:
    # -- Whether to enable or disable the PostgreSQL helm chart.
    enabled: true
    # -- The name override for the Customizer PostgreSQL database.
    nameOverride: customizerdb
    image:
      repository: bitnamilegacy/postgresql
    volumePermissions:
      image:
        repository: bitnamilegacy/os-shell
    metrics:
      image:
        repository: bitnamilegacy/postgres-exporter
    auth:
      # -- Whether to assign a password to the "postgres" admin user. Otherwise, remote access will be blocked for this user.
      enablePostgresUser: true
      # -- Name for a custom user to create.
      username: nemo
      # -- Password for the custom user to create.
      password: nemo
      # -- Name for a custom database to create.
      database: finetuning
      # -- Name of existing secret to use for PostgreSQL credentials.
      existingSecret: ""
    # -- PostgreSQL architecture (`standalone` or `replication`).
    architecture: standalone
    serviceAccount:
      name: customizer-postgresql
      # -- Specifies whether to create a new service account for PostgreSQL.
      create: true

  # -- External PostgreSQL configuration.
  # @default -- This object has the following default values for the external PostgreSQL configuration.
  externalDatabase:
    # -- External database host address.
    host: localhost
    # -- External database port number.
    port: 5432
    # -- Database username for the NeMo Customizer microservice.
    user: nemo
    # -- Name of the database to use.
    database: finetuning
    # -- Name of an existing secret resource containing the database credentials.
    existingSecret: ""
    # -- Name of an existing secret key containing the database credentials.
    existingSecretPasswordKey: ""
    uriSecret:
      name: ""
      key: ""

  # -- WandB configuration.
  # @default -- This object has the following default values for the WandB configuration.
  wandb:
    # -- WandB secret value. Must contain exactly 32 alphanumeric characters. Creates a new Kubernetes secret named "wandb-secret" with key-value pair "encryption_key=<wandb.secretValue>". Ignored if wandb.existingSecret is set.
    secretValue: ec60d96b639764ccf9859bc10d4363d1
    # -- Name of an existing Kubernetes secret resource for the WandB encryption secret.
    existingSecret: ""
    # -- Name of the key in the existing WandB secret containing the secret value. The secret value must be exactly 32 alphanumeric characters: ^[a-zA-Z0-9]{32}$
    existingSecretKey: ""

  # -- Open Telemetry Collector configuration.
  # @default -- This object has the following default values for the Open Telemetry Collector configuration.
  opentelemetry-collector:
    # -- Switch to enable or disable Open Telemetry Collector.
    enabled: true
    image:
      # -- Repository for Open Telemetry Collector image.
      repository: "otel/opentelemetry-collector-k8s"
      # -- Overrides the image tag whose default is the chart appVersion.
      tag: "0.102.1"
    # -- Deployment mode for Open Telemetry Collector. Valid values are "daemonset", "deployment", and "statefulset".
    mode: deployment
    # -- Base collector configuration for Open Telemetry Collector.
    config:
      receivers:
        otlp:
          protocols:
            grpc: {}
            http:
              cors:
                allowed_origins:
                  - "*"
      exporters:
        debug:
          verbosity: detailed
      extensions:
        health_check: {}
        zpages:
          endpoint: 0.0.0.0:55679
      processors:
        batch: {}
      service:
        extensions: [zpages, health_check]
        pipelines:
          traces:
            receivers: [otlp]
            exporters: [debug]
            processors: [batch]
          metrics:
            receivers: [otlp]
            exporters: [debug]
            processors: [batch]
          logs:
            receivers: [otlp]
            exporters: [debug]
            processors: [batch]

  # -- Enable or disable RunAI executor.
  useRunAIExecutor: false

  # -- Deployment configurations for AWS
  awsDeploy:
    # -- Switch on if using AWS and kyverno is installed
    enabled: false
    # -- EFA number of devices per GPU
    efaDevicesPerGPU: 4
  # -- List of model configurations supported by the Customizer.
  # @default -- This object has the following default values.
  customizationTargets:
    # Allow model downloads from Hugging Face
    hfTargetDownload:
      # -- set this to true to allow model downloads from Hugging Face. If enabled=false, models are not allwed to be downloaded from any Hugging Face org and allowedHfOrgs is disregarded
      enabled: false
      # -- List of allowed organizations for model downloads from Hugging Face. Empty list allows all organizations.
      # Example:
      # allowedHfOrgs:
      #   - "nvidia"
      allowedHfOrgs: []
    trustedModelURIs:
      - hf://nvidia/Llama-3_3-Nemotron-Super-49B-v1_5
      - hf://nvidia/Llama-3_3-Nemotron-Super-49B-v1
      - hf://nvidia/NVIDIA-Nemotron-Nano-9B-v2
      - hf://nvidia/NVIDIA-Nemotron-Nano-12B-v2
      - hf://nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16

    # -- Whether to have this values file override targets in the database on application start
    overrideExistingTargets: true
    # -- The default targets to populate the database with
    # @default -- This object has the following default values.
    targets:
      # -- Llama 3.2 1B target model configuration.
      # @default -- This object has the following default values for the Llama 3.2 1B model.
      meta/llama-3.2-1b@2.0:
        # -- The name for target model.
        name: llama-3.2-1b@2.0
        # -- The namespace for target model.
        namespace: meta
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI for Llama 3.2 1B model.
        model_uri: ngc://nvidia/nemo/llama-3_2-1b:2.0
        # -- Path where model files are stored.
        model_path: llama32_1b_2_0
        # -- Mapping to the model name in NIM and Recipe Selection in Customizer. Defaults to being the same as the the configuration entry namespace/name.
        base_model: meta/llama-3.2-1b
        # -- Number of model parameters.
        num_parameters: 1000000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- Llama 3.2 1B Instruct model target configuration.
      # @default -- This object has the following default values for the Llama 3.2 1B Instruct model.
      meta/llama-3.2-1b-instruct@2.0:
        # -- The name for target model.
        name: llama-3.2-1b-instruct@2.0
        # -- The namespace for target model.
        namespace: meta
        # -- Whether to enable the model.
        enabled: true
        # -- NGC model URI
        model_uri: ngc://nvidia/nemo/llama-3_2-1b-instruct:2.0
        # -- Path where model files are stored.
        model_path: llama32_1b-instruct_2_0
        # -- Mapping to the model name in NIM and Recipe Selection in Customizer. Defaults to being the same as the the configuration entry namespace/name.
        base_model: meta/llama-3.2-1b-instruct
        # -- Number of model parameters.
        num_parameters: 1000000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- Llama 3.2 3B Instruct model target configuration.
      # @default -- This object has the following default values for the Llama 3.2 3B Instruct model.
      meta/llama-3.2-3b-instruct@2.0:
        # -- The name for target model.
        name: llama-3.2-3b-instruct@2.0
        # -- The namespace for target model.
        namespace: meta
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI.
        model_uri: ngc://nvidia/nemo/llama-3_2-3b-instruct:2.0
        # -- Path where model files are stored.
        model_path: llama32_3b-instruct_2_0
        # -- Mapping to the model name in NIM and Recipe Selection in Customizer. Defaults to being the same as the the configuration entry namespace/name.
        base_model: meta/llama-3.2-3b-instruct
        # -- Number of model parameters.
        num_parameters: 3000000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- Llama 3 70B Instruct model target configuration.
      # @default -- This object has the following default values for the Llama 3 70B Instruct model.
      meta/llama3-70b-instruct@2.0:
        # -- The name for target model.
        name: llama3-70b-instruct@2.0
        # -- The namespace for target model.
        namespace: meta
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI for Llama 3 70B Instruct model.
        model_uri: ngc://nvidia/nemo/llama-3-70b-instruct-nemo:2.0
        # -- Path where model files are stored.
        model_path: llama-3-70b-bf16_2_0
        # -- Mapping to the model name in NIM and Recipe Selection in Customizer. Defaults to being the same as the the configuration entry namespace/name.
        base_model: meta/llama3-70b-instruct
        # -- Number of model parameters.
        num_parameters: 70000000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- Llama 3.1 8B Instruct model target configuration.
      # @default -- This object has the following default values for the Llama 3.1 8B Instruct model.
      meta/llama-3.1-8b-instruct@2.0:
        # -- The name for target model.
        name: llama-3.1-8b-instruct@2.0
        # -- The namespace for target model.
        namespace: meta
        # -- Whether to enable the model.
        enabled: true
        # -- NGC model URI for Llama 3.1 8B Instruct model.
        model_uri: ngc://nvidia/nemo/llama-3_1-8b-instruct-nemo:2.0
        # -- Path where model files are stored.
        model_path: llama-3_1-8b-instruct_2_0
        # -- Mapping to the model name in NIM and Recipe Selection in Customizer. Defaults to being the same as the the configuration entry namespace/name.
        base_model: meta/llama-3.1-8b-instruct
        # -- Number of model parameters.
        num_parameters: 8000000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- Llama 3.1 70B Instruct model target configuration.
      # @default -- This object has the following default values for the Llama 3.1 70B Instruct model.
      meta/llama-3.1-70b-instruct@2.0:
        # -- The name for target model.
        name: llama-3.1-70b-instruct@2.0
        # -- The namespace for target model.
        namespace: meta
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI for Llama 3.1 70B Instruct model.
        model_uri: ngc://nvidia/nemo/llama-3_1-70b-instruct-nemo:2.0
        # -- Path where model files are stored.
        model_path: llama-3_1-70b-instruct_2_0
        # -- Mapping to the model name in NIM and Recipe Selection in Customizer. Defaults to being the same as the the configuration entry namespace/name.
        base_model: meta/llama-3.1-70b-instruct
        # -- Number of model parameters.
        num_parameters: 70000000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- Phi-4 model target configuration.
      # @default -- This object has the following default values for the Phi-4.
      microsoft/phi-4@1.0:
        # -- The name for target model.
        name: phi-4@1.0
        # -- The namespace for target model.
        namespace: microsoft
        # -- The version for target model.
        version: "1.0"
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI for Phi-4 model.
        model_uri: ngc://nvidia/nemo/phi-4:1.0
        # -- Path where model files are stored.
        model_path: phi-4_1_0
        # -- Mapping to the model name in NIM and Recipe Selection in Customizer. Defaults to being the same as the the configuration entry namespace/name.
        base_model: microsoft/phi-4
        # -- Number of model parameters.
        num_parameters: 14659507200
        # -- Model precision format.
        precision: bf16

      # -- Llama 3.3 70B Instruct model target configuration.
      # @default -- This object has the following default values for the Llama 3.3 70B Instruct model.
      meta/llama-3.3-70b-instruct@2.0:
        # -- The name for target model.
        name: llama-3.3-70b-instruct@2.0
        # -- The namespace for target model.
        namespace: meta
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI for Llama 3.3 70B Instruct model.
        model_uri: ngc://nvidia/nemo/llama-3_3-70b-instruct:2.0
        # -- Path where model files are stored.
        model_path: llama-3_3-70b-instruct_2_0
        # -- Mapping to the model name in NIM and Recipe Selection in Customizer. Defaults to being the same as the the configuration entry namespace/name.
        base_model: meta/llama-3.3-70b-instruct
        # -- Number of model parameters.
        num_parameters: 70000000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- Nemotron Nano Llama 3.1 8B Instruct model target configuration.
      # @default -- This object has the following default values for the Nemotron Nano Llama 3.1 8B Instruct model.
      nvidia/nemotron-nano-llama-3.1-8b@1.0:
        # -- The name for target model.
        name: nemotron-nano-llama-3.1-8b@1.0
        # -- The namespace for target model.
        namespace: nvidia
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI
        model_uri: ngc://nvidia/nemo/nemotron-nano-3_1-8b:0.0.1
        # -- Path where model files are stored.
        model_path: nemotron-nano-3_1-8b_0_0_1
        # -- Mapping to the model name in NIM and Recipe Selection in Customizer. Defaults to being the same as the the configuration entry namespace/name.
        base_model: nvidia/llama-3.1-nemotron-nano-8b-v1
        # -- Number of model parameters.
        num_parameters: 8000000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- Nemotron Super Llama 3.3 49B Instruct model target configuration.
      # @default -- This object has the following default values for the Nemotron Super Llama 3.3 49B Instruct model.
      nvidia/nemotron-super-llama-3.3-49b@1.0:
        # -- The name for target model.
        name: nemotron-super-llama-3.3-49b@1.0
        # -- The namespace for target model.
        namespace: nvidia
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI
        model_uri: ngc://nvidia/nemo/nemotron-super-3_3-49b:v1
        # -- Path where model files are stored.
        model_path: nemotron-super-3_3-49b_v1
        # -- Mapping to the model name in NIM and Recipe Selection in Customizer. Defaults to being the same as the the configuration entry namespace/name.
        base_model: nvidia/llama-3.3-nemotron-super-49b-v1
        # -- Number of model parameters.
        num_parameters: 4900000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- Nemotron Super Llama 3.3 49B v1.5 Instruct model target configuration.
      # @default -- This object has the following default values for the Nemotron Super Llama 3.3 49B v1.5 Instruct model.
      nvidia/nemotron-super-llama-3.3-49b@1.5:
        # -- The name for target model.
        name: nemotron-super-llama-3.3-49b@1.5
        # -- The namespace for target model.
        namespace: nvidia
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI
        model_uri: hf://nvidia/Llama-3_3-Nemotron-Super-49B-v1_5
        # -- Endpoint for where to find this model
        hf_endpoint: https://huggingface.co
        # -- Path where model files are stored.
        model_path: nemotron-super-3_3-49b_v1_5
        # -- Mapping to the model name in NIM and Recipe Selection in Customizer. Defaults to being the same as the the configuration entry namespace/name.
        base_model: nvidia/llama-3.3-nemotron-super-49b-v1.5
        # -- Number of model parameters.
        num_parameters: 4900000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- Llama 3.2 1B embedding target model configuration.
      # @default -- This object has the following default values for the Llama 3.2 1B embedding model.
      nvidia/llama-3.2-nv-embedqa-1b@v2:
        # -- The name for target model.
        name: llama-3.2-nv-embedqa-1b@v2
        # -- The namespace for target model.
        namespace: nvidia
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI
        model_uri: ngc://nvidia/nemo/llama-3_2-1b-embedding-base:0.0.1
        # -- Path where model files are stored.
        model_path: llama32_1b-embedding
        # -- Mapping to the model name to the optimized llama embedding training script and NIM
        base_model: nvidia/llama-3.2-nv-embedqa-1b-v2
        # -- Number of model parameters.
        num_parameters: 1000000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- openai/gpt-oss-20b target model configuration.
      # @default -- This object has the following default values for the openai/gpt-oss-20b model.
      openai/gpt-oss-20b@v1:
        # -- The name for target model.
        name: gpt-oss-20b@v1
        # -- The namespace for target model.
        namespace: openai
        # -- Whether to enable the model.
        enabled: false
        # -- HF model URI
        model_uri: hf://openai/gpt-oss-20b
        # -- HF endpoint
        hf_endpoint: https://huggingface.co
        # -- Path where model files are stored.
        model_path: gpt_oss_20b
        # -- Mapping to the model name to the optimized llama embedding training script and NIM
        base_model: openai/gpt-oss-20b
        # -- Number of model parameters.
        num_parameters: 21000000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- mistralai/Ministral-3-3B-Reasoning-2512 target model configuration.
      # @default -- This object has the following default values for the mistralai/Ministral-3-3B-Reasoning-2512.
      mistralai/ministral-3-3B-reasoning-2512@v1:
        # -- The name for target model.
        name: ministral-3-3B-reasoning-2512@v1
        # -- The namespace for target model.
        namespace: mistralai
        # -- Whether to enable the model.
        enabled: false
        # -- HF model URI
        model_uri: hf://mistralai/Ministral-3-3B-Reasoning-2512
        # -- HF endpoint
        hf_endpoint: https://huggingface.co
        # -- Path where model files are stored.
        model_path: mistral-3-3b-reasoning-2512
        # -- Mapping to the model name to the optimized llama embedding training script and NIM
        base_model: mistralai/Ministral-3-3B-Reasoning-2512
        # -- Number of model parameters.
        num_parameters: 3400000000
        # -- Model precision format.
        precision: bf16-mixed
        
      # -- nvidia/nemotron-nano-9b-v2 target model configuration.
      # @default -- This object has the following default values for the nvidia/nemotron-nano-9b-v2 model.
      nvidia/nemotron-nano-9b-v2@v1:
        # -- The name for target model.
        name: nemotron-nano-9b-v2@v1
        # -- The namespace for target model.
        namespace: nvidia
        # -- Whether to enable the model.
        enabled: false
        # -- HF model URI
        model_uri: hf://nvidia/NVIDIA-Nemotron-Nano-9B-v2
        # -- HF endpoint
        hf_endpoint: https://huggingface.co
        # -- Path where model files are stored.
        model_path: nemotron_nano_9b_v2
        # -- Mapping to the model name to the optimized llama embedding training script and NIM
        base_model: nvidia/nemotron-nano-9b-v2
        # -- Number of model parameters.
        num_parameters: 9000000000
        # -- Model precision format.
        precision: bf16-mixed

  # -- List of customization configuration template supported by the Customizer.
  # @default -- This object has the following default values.
  customizationConfigTemplates:
    # -- Whether to have this values file override templates in the database on application start
    overrideExistingTemplates: true
    # -- The default templates to populate the database with
    # @default -- This object has the following default values.
    templates:
      # -- Llama 3.2 3B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.2 3B Instruct model.
      meta/llama-3.2-3b-instruct@v1.0.0+80GB:
        # -- The name for training config template.
        name: llama-3.2-3b-instruct@v1.0.0+80GB
        # -- The namespace for training config template.
        namespace: meta
        # -- The target to perform the customization on.
        target: meta/llama-3.2-3b-instruct@2.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3.2 1B model configuration.
      # @default -- This object has the following default values for the Llama 3.2 1B model.
      meta/llama-3.2-1b@v1.0.0+80GB:
        # -- The name for training config template.
        name: llama-3.2-1b@v1.0.0+80GB
        # -- The namespace for training config template.
        namespace: meta
        # -- The target to perform the customization on.
        target: meta/llama-3.2-1b@2.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size:
              1
              # -- Training method.1
          - training_type: sft
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3.2 1B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.2 1B Instruct model.
      meta/llama-3.2-1b-instruct@v1.0.0+80GB:
        # -- The name for training config template.
        name: llama-3.2-1b-instruct@v1.0.0+80GB
        # -- The namespace for training config template.
        namespace: meta
        # -- The target to perform the customization on.
        target: meta/llama-3.2-1b-instruct@2.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size:
              1
              # -- Training method.1
          - training_type: sft
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
          - training_type: dpo
            # -- The type of fine-tuning method.
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3 70B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3 70B Instruct model.
      meta/llama3-70b-instruct@v1.0.0+80GB:
        # -- The name for training config template.
        name: llama3-70b-instruct@v1.0.0+80GB
        # -- The namespace for training config template.
        namespace: meta
        # -- Resource configuration for each training option for the target model.
        target: meta/llama3-70b-instruct@2.0
        # -- Training options for different fine-tuning methods.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3.1 8B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.1 8B Instruct model.
      meta/llama-3.1-8b-instruct@v1.0.0+80GB:
        # -- The name for training config template.
        name: llama-3.1-8b-instruct@v1.0.0+80GB
        # -- The namespace for training config template.
        namespace: meta
        # -- Resource configuration for each training option for the target model.
        target: meta/llama-3.1-8b-instruct@2.0
        # -- Training options for different fine-tuning methods.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size:
              1
              # -- Training method.1
          - training_type: sft
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 8
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of GPUs among which the models layers are distributed.
            pipeline_parallel_size: 2
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
          - training_type: dpo
            # -- The type of fine-tuning method.
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 8
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 8
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3.1 70B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.1 70B Instruct model.
      meta/llama-3.1-70b-instruct@v1.0.0+80GB:
        # -- The name for training config template.
        name: llama-3.1-70b-instruct@v1.0.0+80GB
        # -- The namespace for training config template.
        namespace: meta
        # -- Resource configuration for each training option for the target model.
        target: meta/llama-3.1-70b-instruct@2.0
        # -- Training options for different fine-tuning methods.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Phi-4 model configuration.
      # @default -- This object has the following default values for the Phi-4.
      microsoft/phi-4@v1.0.0+80GB:
        # -- The name for training config template.
        name: phi-4@v1.0.0+80GB
        # -- The namespace for training config template.
        namespace: microsoft
        # -- The target to perform the customization on.
        target: microsoft/phi-4@1.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3.3 70B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.3 70B Instruct model.
      meta/llama-3.3-70b-instruct@v1.0.0+80GB:
        # -- The name for training config template.
        name: llama-3.3-70b-instruct@v1.0.0+80GB
        # -- The namespace for training config template.
        namespace: meta
        # -- The target to perform the customization on.
        target: meta/llama-3.3-70b-instruct@2.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Nemotron Nano Llama 3.1 8B Instruct model configuration.
      # @default -- This object has the following default values for the Nemotron Nano Llama 3.1 8B Instruct model.
      nvidia/nemotron-nano-llama-3.1-8b@v1.0.0+80GB:
        # -- The name for training config template.
        name: nemotron-nano-llama-3.1-8b@v1.0.0+80GB
        # -- The namespace for training config template.
        namespace: nvidia
        # -- The target to perform the customization on.
        target: nvidia/nemotron-nano-llama-3.1-8b@1.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 8
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of GPUs among which the models layers are distributed.
            pipeline_parallel_size: 2
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      nvidia/llama-3.2-nv-embedqa-1b@v2+80GB:
        # -- The name for training config template.
        name: llama-3.2-nv-embedqa-1b@v2+80GB
        # -- The namespace for training config template.
        namespace: nvidia
        # -- The target to perform the customization on.
        target: nvidia/llama-3.2-nv-embedqa-1b@v2
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora_merged
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1

        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 2048
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Nemotron Super Llama 3.3 49B Instruct model configuration.
      # @default -- This object has the following default values for the Nemotron Super Llama 3.3 49B Instruct model.
      nvidia/nemotron-super-llama-3.3-49b@v1.0.0+80GB:
        # -- The name for training config template.
        name: nemotron-super-llama-3.3-49b@v1.0.0+80GB
        # -- The namespace for training config template.
        namespace: nvidia
        # -- The target to perform the customization on.
        target: nvidia/nemotron-super-llama-3.3-49b@1.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Nemotron Super Llama 3.3 49B v1.5 Instruct model configuration.
      # @default -- This object has the following default values for the Nemotron Super Llama 3.3 49B v1.5 Instruct model.
      nvidia/nemotron-super-llama-3.3-49b@v1.5+80GB:
        # -- The name for training config template.
        name: nemotron-super-llama-3.3-49b@v1.5+80GB
        # -- The namespace for training config template.
        namespace: nvidia
        # -- The target to perform the customization on.
        target: nvidia/nemotron-super-llama-3.3-49b@1.5
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Mistral 3.3B Reasoning 2512 model configuration.
      # @default -- This object has the following default values for the Mistral 3.3B Reasoning 2512 model.
      mistralai/ministral-3-3B-reasoning-2512@v1.0.0+80GB:
        # -- The name for training config template.
        name: ministral-3-3B-reasoning-2512@v1.0.0+80GB
        # -- The namespace for training config template.
        namespace: mistralai
        # -- The target to perform the customization on.
        target: mistralai/ministral-3-3B-reasoning-2512@v1
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size:
              1
              # -- Training method.1
          - training_type: sft
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      #### ------------------------------ Nvidia L40 GPU default configurations ------------------------------ ####
      # -- Llama 3.2 1B model configuration.
      # @default -- This object has the following default values for the Llama 3.2 1B model.
      meta/llama-3.2-1b@v1.0.0+40GB:
        # -- The name for training config template.
        name: llama-3.2-1b@v1.0.0+40GB
        # -- The namespace for training config template.
        namespace: meta
        # -- The target to perform the customization on.
        target: meta/llama-3.2-1b@2.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
            # -- Training method.1
          - training_type: sft
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3.2 1B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.2 1B Instruct model.
      meta/llama-3.2-1b-instruct@v1.0.0+40GB:
        # -- The name for training config template.
        name: llama-3.2-1b-instruct@v1.0.0+40GB
        # -- The namespace for training config template.
        namespace: meta
        # -- The target to perform the customization on.
        target: meta/llama-3.2-1b-instruct@2.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
            # -- Training method.1
          - training_type: sft
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
          - training_type: dpo
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 2
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 2
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      nvidia/llama-3.2-nv-embedqa-1b@v2+40GB:
        # -- The name for training config template.
        name: llama-3.2-nv-embedqa-1b@v2+40GB
        # -- The namespace for training config template.
        namespace: nvidia
        # -- The target to perform the customization on.
        target: nvidia/llama-3.2-nv-embedqa-1b@v2
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 2048

      # -- Llama 3.2 3B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.2 3B Instruct model.
      meta/llama-3.2-3b-instruct@v1.0.0+40GB:
        # -- The name for training config template.
        name: llama-3.2-3b-instruct@v1.0.0+40GB
        # -- The namespace for training config template.
        namespace: meta
        # -- The target to perform the customization on.
        target: meta/llama-3.2-3b-instruct@2.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3.1 8B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.1 8B Instruct model.
      meta/llama-3.1-8b-instruct@v1.0.0+40GB:
        # -- The name for training config template.
        name: llama-3.1-8b-instruct@v1.0.0+40GB
        # -- The namespace for training config template.
        namespace: meta
        # -- Resource configuration for each training option for the target model.
        target: meta/llama-3.1-8b-instruct@2.0
        # -- Training options for different fine-tuning methods.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 2
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 2
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
            # -- Training method.1
          - training_type: sft
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 2
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of GPUs among which the models layers are distributed.
            pipeline_parallel_size: 2
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
          - training_type: dpo
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 2
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 8
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3 70B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3 70B Instruct model.
      meta/llama3-70b-instruct@v1.0.0+40GB:
        # -- The name for training config template.
        name: llama3-70b-instruct@v1.0.0+40GB
        # -- The namespace for training config template.
        namespace: meta
        # -- Resource configuration for each training option for the target model.
        target: meta/llama3-70b-instruct@2.0
        # -- Training options for different fine-tuning methods.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 2
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of GPUs among which the models layers are distributed.
            pipeline_parallel_size: 2
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3.1 70B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.1 70B Instruct model.
      meta/llama-3.1-70b-instruct@v1.0.0+40GB:
        # -- The name for training config template.
        name: llama-3.1-70b-instruct@v1.0.0+40GB
        # -- The namespace for training config template.
        namespace: meta
        # -- Resource configuration for each training option for the target model.
        target: meta/llama-3.1-70b-instruct@2.0
        # -- Training options for different fine-tuning methods.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 2
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of GPUs among which the models layers are distributed.
            pipeline_parallel_size: 2
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3.3 70B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.3 70B Instruct model.
      meta/llama-3.3-70b-instruct@v1.0.0+40GB:
        # -- The name for training config template.
        name: llama-3.3-70b-instruct@v1.0.0+40GB
        # -- The namespace for training config template.
        namespace: meta
        # -- The target to perform the customization on.
        target: meta/llama-3.3-70b-instruct@2.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 2
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of GPUs among which the models layers are distributed.
            pipeline_parallel_size: 2
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Nemotron Nano Llama 3.1 8B Instruct model configuration.
      # @default -- This object has the following default values for the Nemotron Nano Llama 3.1 8B Instruct model.
      nvidia/nemotron-nano-llama-3.1-8b@v1.0.0+40GB:
        # -- The name for training config template.
        name: nemotron-nano-llama-3.1-8b@v1.0.0+40GB
        # -- The namespace for training config template.
        namespace: nvidia
        # -- The target to perform the customization on.
        target: nvidia/nemotron-nano-llama-3.1-8b@1.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 2
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 2
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 2
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of GPUs among which the models layers are distributed.
            pipeline_parallel_size: 2
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1

        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Nemotron Super Llama 3.3 49B Instruct model configuration.
      # @default -- This object has the following default values for the Nemotron Super Llama 3.3 49B Instruct model.
      nvidia/nemotron-super-llama-3.3-49b@v1.0.0+40GB:
        # -- The name for training config template.
        name: nemotron-super-llama-3.3-49b@v1.0.0+40GB
        # -- The namespace for training config template.
        namespace: nvidia
        # -- The target to perform the customization on.
        target: nvidia/nemotron-super-llama-3.3-49b@1.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 2
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of GPUs among which the models layers are distributed.
            pipeline_parallel_size: 2
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Phi-4 model configuration.
      # @default -- This object has the following default values for the Phi-4.
      microsoft/phi-4@v1.0.0+40GB:
        # -- The name for training config template.
        name: phi-4@v1.0.0+40GB
        # -- The namespace for training config template.
        namespace: microsoft
        # -- The target to perform the customization on.
        target: microsoft/phi-4@1.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 2
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Openai gpt-oss 20B model configuration.
      # @default -- This object has the following default values for the OpenAI gpt-oss 20b model.
      openai/gpt-oss-20b@v1.0.0+80GB:
        # -- The name for training config template.
        name: gpt-oss-20b@v1.0.0+80GB
        # -- The namespace for training config template.
        namespace: openai
        # -- The target to perform the customization on.
        target: openai/gpt-oss-20b@v1
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 8
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- Number of GPUs used to split the model across layers for pipeline model.
            pipeline_parallel_size: 1
            # -- Number of GPUs used to parallelize expert (MoE) components of the model.
            expert_model_parallel_size: 8
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- Number of GPUs used to split the model across layers for pipeline model.
            pipeline_parallel_size: 1
            # -- Number of GPUs used to parallelize expert (MoE) components of the model.
            expert_model_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- NVIDIA Nemotron-Nano-V2 9b model configuration.
      # @default -- This object has the following default values for the NVIDIA nemotron-nano-9b-v2 model.
      nvidia/nemotron-nano-9b-v2@v1.0.0+80GB:
        # -- The name for training config template.
        name: nemotron-nano-9b-v2@v1.0.0+80GB
        # -- The namespace for training config template.
        namespace: nvidia
        # -- The target to perform the customization on.
        target: nvidia/nemotron-nano-9b-v2@v1
        # -- Resource configuration for each training option for the target model.
        training_options:
          # uncomment when LoRA profile is available in NIM
          # # -- Training method.
          # - training_type: sft
          #   # -- The type of fine-tuning method.
          #   finetuning_type: lora
          #   # -- The number of GPUs per node to use for the specified training.
          #   num_gpus: 1
          #   # -- The number of nodes to use for the specified training.
          #   num_nodes: 1
          #   # -- The number of GPUs among which the models tensors are partitioned.
          #   tensor_parallel_size: 1
          #   # -- The number of training examples processed in parallel by each individual GPU.
          #   micro_batch_size: 1
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 8
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of GPUs among which the models layers are distributed.
            pipeline_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

  # -- Sets predictable name for the NeMo Customizer Kubernetes service
  serviceName: nemo-customizer

nim:
  # -- Specifies whether to deploy a NIM for LLM during the Helm installation of the chart. You can deploy a single static NIM by enabling this object and its values. When enabled, the chart deploys `meta/llama-3.1-8b-instruct` as the default NIM.
  enabled: false
  image:
    # Adjust to the actual location of the image and version you want
    repository: nvcr.io/nim/meta/llama-3.1-8b-instruct
    tag: "1.8"
  imagePullSecrets:
    - name: nvcrimagepullsecret
  model:
    # -- The name of the model to deploy as NIM.
    name: meta/llama-3.1-8b-instruct # not strictly necessary, but enables running "helm test"
    # -- The NGC API secret for model access.
    ngcAPISecret: ngc-api
  service:
    # -- Labels for the NIM service.
    labels:
      app.nvidia.com/nim-type: inference
  # -- Environment variables for the NIM service.
  env:
    - name: NIM_PEFT_SOURCE
      value: http://nemo-entity-store:8000
    - name: NIM_PEFT_REFRESH_INTERVAL
      value: "30"
    - name: NIM_MAX_CPU_LORAS
      value: "16"
    - name: NIM_MAX_GPU_LORAS
      value: "8"
  persistence:
    # -- Specifies whether to enable persistence volume claim (PVC) for the NIM service.
    enabled: true
    # -- Specifies the storage class to use for the PVC.
    storageClass: ""
    # -- Annotations for the PVC.
    annotations:
      helm.sh/resource-policy: keep
  statefulSet:
    # -- Specifies whether to enable a stateful set for the NIM service.
    enabled: false
  # -- Specifies resources for the NIM service.
  resources:
    limits:
      nvidia.com/gpu: 1
    # -- Specifies requests for the NIM service.
    requests:
      nvidia.com/gpu: 1

evaluator:
  # -- The number of replicas for the NeMo Evaluator microservice.
  replicaCount: 1

  image:
    # -- The image pull policy for the NeMo Evaluator microservice image.
    pullPolicy: IfNotPresent
    # -- The repository where the NeMo Evaluator microservice image is located.
    repository: nvcr.io/nvidia/nemo-microservices/evaluator
    # -- Specifies the version of the NeMo Evaluator microservice image.
    tag: ""

  # -- The image pull secrets for accessing the container registry.
  imagePullSecrets:
    - name: nvcrimagepullsecret

  # -- The name override for the NeMo Evaluator microservice.
  nameOverride: ""

  # -- The full name override for the NeMo Evaluator microservice.
  fullnameOverride: ""

  serviceAccount:
    # -- Whether to create a service account for the NeMo Evaluator microservice.
    create: true
    # -- Whether to automatically mount the service account token.
    automount: true
    # -- Annotations for the service account.
    annotations: {}
    # -- A name for the service account.
    name: ""

  # -- Annotations for the service pod.
  podAnnotations: {}
  # -- Labels for the service pod.
  podLabels: {}

  # -- Security context for the service pod.
  podSecurityContext: {}

  # -- Security context for the service container.
  securityContext: {}

  # -- Additional environment variables to pass to containers. The format is `NAME: value` or `NAME: valueFrom: {object}`.
  env: {}

  service:
    # -- The type of the NeMo Evaluator microservice.
    type: ClusterIP
    # -- External port of the NeMo Evaluator microservice.
    port: 7331
    # -- Internal port of the NeMo Evaluator microservice.
    internalPort: 7332

  evaluationJob:
    # -- Monitoring interval checking evaluation job status (in seconds).
    monitoringInterval: 5
    # -- Monitoring timeout for checking evaluation job status (in seconds).
    monitoringTimeout: 36000

  evalFactory:
    job:
      # -- Restart policy for the pods of the core eval jobs.
      restartPolicy: Never
      # -- Time-to-live after completion for the core eval jobs.
      ttlSecondsAfterFinished: 172800

  # -- Optional override for evaluation images.
  evaluationImages:
    # -- The image for the bigcode evaluation harness evaluation: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness
    BIGCODE_EVALUATION_HARNESS: ""
    # -- The image for the language model evaluation harness evaluation: nvcr.io/nvidia/eval-factory/lm-evaluation-harness
    LM_EVAL_HARNESS: ""
    # -- The image for the retriever evaluation: nvcr.io/nvidia/eval-factory/rag_retriever_eval
    RETRIEVER: ""
    # -- The image for the RAG evaluation: nvcr.io/nvidia/eval-factory/rag_retriever_eval
    RAG: ""
    # -- The image for the BFCL evaluation: nvcr.io/nvidia/eval-factory/bfcl
    BFCL: ""
    # -- The image for the Agentic evaluation: nvcr.io/nvidia/eval-factory/agentic_eval
    AGENTIC_EVAL: ""
    # -- The image for the model safety evaluation: nvcr.io/nvidia/eval-factory/safety-harness
    SAFETY_HARNESS: ""
    # -- The image for the Simple-Evals evaluation: nvcr.io/nvidia/eval-factory/simple-evals
    SIMPLE_EVALS: ""

  # -- Resources for the NeMo Evaluator service.
  resources: {}

  livenessProbe:
    # -- Configures the liveness probe for the NeMo Evaluator microservice. The liveness probe checks if the container is running. The probe sends an HTTP GET request to the `/health` endpoint on the container's `http` port.
    httpGet:
      path: /health
      port: http

  readinessProbe:
    # -- Configures the readiness probe for the NeMo Evaluator microservice. The readiness probe checks if the container is ready to receive traffic. The probe sends an HTTP GET request to the `/health` endpoint on the container's `http` port.
    httpGet:
      path: /health
      port: http

  autoscaling:
    # -- Whether to enable autoscaling for the NeMo Evaluator microservice.
    enabled: false
    # -- The minimum number of replicas for the NeMo Evaluator microservice.
    minReplicas: 1
    # -- The maximum number of replicas for the NeMo Evaluator microservice.
    maxReplicas: 100
    # -- The target CPU utilization percentage for the NeMo Evaluator microservice.
    targetCPUUtilizationPercentage: 80

  # -- Volumes for the NeMo Evaluator microservice.
  volumes: []
  # -- Volume mounts for the NeMo Evaluator microservice.
  volumeMounts: []

  # -- Node selector for the NeMo Evaluator microservice.
  nodeSelector: {}

  # -- Tolerations for the NeMo Evaluator microservice.
  tolerations: []

  # -- Affinity for the NeMo Evaluator microservice.
  affinity: {}

  # -- Host for the NeMo Evaluator microservice.
  evaluator:
    host: "0.0.0.0"

  # -- Whether to enable the OpenTelemetry exporter for the NeMo Evaluator microservice.
  otelExporterEnabled: false

  # -- Log level for the NeMo Evaluator microservice.
  logLevel: INFO

  # -- OpenTelemetry environment configuration variables for the NeMo Evaluator microservice.
  otelEnvVars:
    # OTEL_EXPORTER_OTLP_ENDPOINT: "http://$(HOST_IP):4317" # sends to gRPC receiver on port 4317
    OTEL_SERVICE_NAME: "nemo-evaluator"
    OTEL_TRACES_EXPORTER: otlp
    OTEL_METRICS_EXPORTER: otlp
    OTEL_LOGS_EXPORTER: otlp
    OTEL_PROPAGATORS: "tracecontext,baggage"
    OTEL_RESOURCE_ATTRIBUTES: "deployment.environment=$(NAMESPACE)"
    OTEL_PYTHON_EXCLUDED_URLS: "health"
    OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED: "true"

  external:
    dataStore:
      # -- The external URL of the NeMo Data Store microservice.
      endpoint: "http://nemo-data-store:3000/v1/hf"
    entityStore:
      # -- The external URL of the NeMo Entity Store microservice.
      endpoint: "http://nemo-entity-store:8000"
    jobs:
      # -- The external URL of the Jobs microservice.
      endpoint: "http://nemo-core-api:8000"
    nimProxy:
      # -- The external URL of the NeMo NIM Proxy microservice.
      endpoint: "http://nemo-nim-proxy:8000"
      # -- The internal cluster URL of the NeMo NIM Proxy microservice. This is used to remap model URLs from evaluation jobs that match external.nimProxy.endpoint to the internal URL when cluster ingress is not supported for the external address.
      endpoint_internal: ""
    milvus:
      # -- The external URL of your own Milvus service.
      endpoint: ""

  dbMigration:
    resources:
      limits:
        # -- The CPU resource limit for the DB Migration service.
        cpu: 1
        # -- The memory resource limit for the DB Migration service.
        memory: 1Gi

  postgresql:
    # -- Whether to enable the default PostgreSQL service. For more information about setting up a PostgreSQL service, see the [PostgreSQL setup guide](https://docs.nvidia.com/nemo/microservices/latest/set-up/manage-storage/database/postgres.html).
    enabled: true
    # -- The architecture for the default PostgreSQL service.
    architecture: standalone
    global:
      # -- The storage class for the default PostgreSQL service.
      storageClass: ""
      # -- The storage size for the default PostgreSQL service.
      size: 10Gi
    image:
      repository: bitnamilegacy/postgresql
    volumePermissions:
      image:
        repository: bitnamilegacy/os-shell
    metrics:
      image:
        repository: bitnamilegacy/postgres-exporter
    auth:
      # -- Whether to enable the PostgreSQL user.
      enablePostgresUser: true
      # -- The username for the PostgreSQL service.
      username: nemo
      # -- The password for the PostgreSQL service.
      password: nemo
      # -- The database for the PostgreSQL service.
      database: evaluation
      # -- The existing secret you want to use for the PostgreSQL service.
      existingSecret: ""
    primary:
      networkPolicy:
        # -- Specifies whether to enable the network policy for the PostgreSQL service.
        enabled: false
      service:
        ports:
          # -- The primary service port for the PostgreSQL service.
          postgresql: 5432
    # -- The name override for the default PostgreSQL database.
    nameOverride: evaluatordb
    serviceAccount:
      # -- The name of the service account for PostgreSQL.
      name: evaluator-postgresql

  postgresWaitImage:
    # -- The repository location of the image used to wait for postgres to start.
    repository: "busybox"
    # -- The tag of the image used when waiting.
    tag: "latest"

  externalDatabase:
    # -- The host for an external database.
    host: localhost
    # -- The port for the external database.
    port: 5432
    # -- The user for the external database.
    user: nemo
    # -- The database for the external database.
    database: evaluation
    # -- The existing secret for the external database.
    existingSecret: ""
    # -- The existing secret password key for the external database.
    existingSecretPasswordKey: ""
    # -- The name for the external database secret.
    uriSecret:
      name: ""
      key: ""

  zipkin:
    # -- Whether to enable the default Zipkin service.
    enabled: false
    # -- The full name override for the default Zipkin service.
    fullnameOverride: "evaluator-zipkin"

  opentelemetry-collector:
    # -- Whether to enable the OpenTelemetry Collector service.
    enabled: false
    # -- The mode for the OpenTelemetry Collector service.
    mode: deployment
    config:
      receivers:
        # -- The OTLP receiver for the OpenTelemetry Collector service.
        otlp:
          protocols:
            grpc:
            http:
              cors:
                allowed_origins:
                  - "*"
      exporters:
        # NOTE: Prior to v0.86.0 use `logging` instead of `debug`.
        # -- The Zipkin exporter for the OpenTelemetry Collector service.
        zipkin:
          endpoint: "http://zipkin:9411/api/v2/spans"
        # -- Debugging verbosity for the OpenTelemetry Collector service.
        debug:
          verbosity: detailed
      processors:
        # -- The batch processor for the OpenTelemetry Collector service.
        batch: {}
        # -- The tail sampling processor for the OpenTelemetry Collector service.
        tail_sampling:
          # filter out health checks
          # https://github.com/open-telemetry/opentelemetry-collector/issues/2310#issuecomment-1268157484
          # -- The policies for the OpenTelemetry Collector service.
          policies:
            - name: drop_noisy_traces_url
              type: string_attribute
              string_attribute:
                key: http.target
                values:
                  - \/health
                enabled_regex_matching: true
                invert_match: true
        # -- The transform processor configuration for the OpenTelemetry Collector service.
        transform:
          # -- The trace statements for the OpenTelemetry Collector service.
          trace_statements:
            - context: span
              statements:
                - set(status.code, 1) where attributes["http.path"] == "/health"
                # here, you can add code to replace span names or attributes for your own needs.
      service:
        pipelines:
          # -- The traces pipeline for the OpenTelemetry Collector service.
          traces:
            # -- The receivers for the traces pipeline for the OpenTelemetry Collector service.
            receivers: [otlp]
            # -- The exporters for the traces pipeline for the OpenTelemetry Collector service.
            exporters: [debug, zipkin]
            # -- The processors for the traces pipeline for the OpenTelemetry Collector service.
            processors: [tail_sampling, transform]
          # -- The metrics pipeline for the OpenTelemetry Collector service.
          metrics:
            # -- The receivers for the metrics pipeline for the OpenTelemetry Collector service.
            receivers: [otlp]
            # -- The exporters for the metrics pipeline for the OpenTelemetry Collector service.
            exporters: [debug]
            # -- The processors for the metrics pipeline for the OpenTelemetry Collector service.
            processors: [batch]
          # -- The logs pipeline for the OpenTelemetry Collector service.
          logs:
            # -- The receivers for the logs pipeline for the OpenTelemetry Collector service.
            receivers: [otlp]
            # -- The exporters for the logs pipeline for the OpenTelemetry Collector service.
            exporters: [debug]
            # -- The processors for the logs pipeline for the OpenTelemetry Collector service.
            processors: [batch]

  milvus:
    # -- Whether to enable the default Milvus service. Enable this for RAG and Retriever pipelines. For more information about setting up a Milvus service, see the [Milvus setup guide](https://docs.nvidia.com/nemo/microservices/latest/set-up/deploy-as-microservices/evaluator.html#configure-milvus).
    enabled: false
    # -- The service name for the default Milvus service.
    serviceName: milvus
    service:
      # -- The service port for the default Milvus service.
      port: 19121
    cluster:
      # -- Whether to enable the default Milvus cluster.
      enabled: false
    etcd:
      # -- Whether to enable the etcd for the default Milvus service.
      enabled: false
    pulsar:
      # -- Whether to enable the Pulsar for the default Milvus service.
      enabled: false
    minio:
      # -- Whether to enable the Minio for the default Milvus service.
      enabled: false
      # -- Whether to enable the TLS for the Minio service.
      tls:
        enabled: false
    standalone:
      # -- Whether to enable the standalone for the default Milvus service.
      persistence:
        # -- Whether to enable the persistence for the default Milvus service.
        enabled: true
        # -- The persistent volume claim for the default Milvus service.
        persistentVolumeClaim:
          # -- The size for the persistent volume claim for the default Milvus service.
          size: 50Gi
          # -- The storage class for the persistent volume claim for the default Milvus service.
          storageClass: ""
      # -- Extra environment variables for the default Milvus service.
      extraEnv:
        - name: LOG_LEVEL
          value: error
    # -- Extra configuration files for the default Milvus service.
    extraConfigFiles:
      user.yaml: |+
        etcd:
          use:
            embed: true
          data:
            dir: /var/lib/milvus/etcd
        common:
          storageType: local

  # -- Sets a predictable name for the NeMo Evaluator Kubernetes service
  serviceName: nemo-evaluator

guardrails:
  # -- Number of replicas for the NeMo Guardrails microservice deployment.
  replicaCount: 1

  image:
    # -- The repository location of the NeMo Guardrails container image.
    repository: nvcr.io/nvidia/nemo-microservices/guardrails
    # -- The tag of the NeMo Guardrails container image.
    tag: ""
    # -- The image pull policy for the NeMo Guardrails container image.
    pullPolicy: IfNotPresent

  guardrailsExtProc:
    enabled: false
    extProcImage:
      # -- Repository for Guardrails Callout server image.
      repository: nvcr.io/nvidia/nemo-microservices/guardrails-callout
      # -- The tag of the NeMo Guardrails Callout server image.
      tag: ""
      # -- Image pull policy for Guardrails Callout server image.
      imagePullPolicy: IfNotPresent
    env:
      GRPC_GO_LOG_SEVERITY_LEVEL: "INFO"
      GR_GRPC__TLS__ENABLED: "true"
      GR_GRPC__TLS__CERT_FILE: "/ssl_creds/server.crt"
      GR_GRPC__TLS__KEY_FILE: "/ssl_creds/server.key"
      OTEL_SERVICE_NAME: "guardrails-ext-proc-service"
      OTEL_EXPORTER_OTLP_PROTOCOL: "grpc"
      OTEL_EXPORTER_OTLP_INSECURE: "true"
  # -- Specifies the list of secret names that are needed for the main container and any init containers.
  imagePullSecrets:
    - name: nvcrimagepullsecret

  # -- Overrides the chart name.
  nameOverride: ""

  # -- Overrides the full chart name.
  fullnameOverride: ""

  # -- Kubernetes secret containing NVIDIA_API_KEY for Guardrails to use Nemoguard NIMS on NVCF
  guardrails:
    nvcfAPIKeySecretName: ""

  # -- Environment variables for the container.
  env:
    # -- The NIM endpoint URL for the NeMo Guardrails microservice.
    NIM_ENDPOINT_URL: http://nemo-nim-proxy:8000/v1
    CONFIG_STORE_PATH: "/app/services/guardrails/config-store"
    DEMO: "True"
    DEFAULT_CONFIG_ID: self-check
    DEFAULT_LLM_PROVIDER: "nim"
    NEMO_GUARDRAILS_SERVER_ENABLE_CORS: "False"
    NEMO_GUARDRAILS_SERVER_ALLOWED_ORIGINS: "*"
    FETCH_NIM_APP_MODELS: "True"
    GUARDRAILS_HOST: "0.0.0.0"
    GUARDRAILS_PORT: "7331"
    # Need to add this in MS specic env vars because we need different values for Guardrails MS and ext_proc service
    OTEL_SERVICE_NAME: "nemo-guardrails"

  # -- Whether to enable the OpenTelemetry exporter for the NeMo Guardrails microservice.
  otelExporterEnabled: false

  # -- Log level for the NeMo Guardrails microservice.
  logLevel: INFO

  # -- OpenTelemetry environment configuration variables for the NeMo Guardrails microservice.
  otelEnvVars:
    # Uncomment to use a custom OpenTelemetry collector endpoint.
    # OTEL_EXPORTER_OTLP_ENDPOINT: "http://$(HOST_IP):4317" # sends to gRPC receiver on port 4317
    OTEL_TRACES_EXPORTER: otlp
    OTEL_METRICS_EXPORTER: otlp
    OTEL_LOGS_EXPORTER: otlp
    OTEL_PROPAGATORS: "tracecontext,baggage"
    OTEL_PYTHON_EXCLUDED_URLS: "health"
    OTEL_RESOURCE_ATTRIBUTES: "deployment.environment=$(NAMESPACE),service.namespace=$(NAMESPACE)"

  # -- External service endpoints configuration.
  external:
    entityStore:
      # -- The external URL of the NeMo Entity Store microservice.
      endpoint: "http://nemo-entity-store:8000"

  # -- Specifies the service type and the port for the deployment.
  service:
    type: ClusterIP
    port: 7331

  configStore:
    nfs:
      # -- Whether to enable the use of an NFS persistent volume for the configuration store.
      enabled: false
      # -- The path to the root of the Configuration Store folder.
      path: "/path/to/nfs/share"
      # -- The address of the NFS server.
      server: "nfs-server.example.com"
      # -- The path where the NFS volume will be mounted inside the container.
      mountPath: "/config-store"
      # -- The storage class for the PV and PVC.
      storageClass: "standard"

  readinessProbe:
    # -- The HTTP GET request to use for the readiness probe.
    httpGet:
      path: /v1/health
      port: 7331
    # -- The initial delay seconds for the readiness probe.
    initialDelaySeconds: 5
    # -- The timeout in seconds for the readiness probe.
    timeoutSeconds: 30

  livenessProbe:
    # -- The HTTP GET request to use for the liveness probe.
    httpGet:
      path: /v1/health
      port: 7331
    # -- The initial delay seconds for the liveness probe.
    initialDelaySeconds: 5
    # -- The timeout in seconds for the liveness probe.
    timeoutSeconds: 30

  serviceAccount:
    # -- Whether to create a service account for the NeMo Guardrails microservice.
    create: true
    # -- Whether to automatically mount the service account token.
    automount: true
    # -- Annotations to be added to the service account.
    annotations: {}
    # -- The name of the service account to use.
    name: ""

  # -- Specifies additional annotations to the main deployment pods.
  podAnnotations: {}

  # -- Specifies additional labels to the main deployment pods.
  podLabels:
    hpe-ezua/type: "vendor-service"
    hpe-ezua/app: "nemo-microservices"

  # -- Specifies privilege and access control settings for the pod.
  podSecurityContext: {}
  # -- Specifies the group ID for the pod.
  runAsGroup: ""
  # -- Specifies the file system owner group id.
  fsGroup: ""

  # -- Specifies security context for the container.
  securityContext: {}

  # -- Specifies resource configurations for the deployment.
  resources:
    limits:
      cpu: 8000m
      memory: 16Gi
    requests:
      cpu: 100m
      memory: 128Mi
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  # -- Specifies autoscaling configurations for the deployment.
  autoscaling:
    # -- Whether to enable horizontal pod autoscaler.
    enabled: false
    # -- The minimum number of replicas for the deployment.
    minReplicas: 1
    # -- The maximum number of replicas for the deployment.
    maxReplicas: 100
    # -- The target CPU utilization percentage.
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

    # -- PostgreSQL configuration for the NeMo Guardrails microservice.
  postgresql:
    # -- Whether to install the default PostgreSQL Helm chart. If enabled, the NeMo Guardrails microservice Helm chart uses the [PostgreSQL Helm chart from Bitnami](https://github.com/bitnami/charts/blob/main/bitnami/postgresql/values.yaml) to create a PostgreSQL database.
    enabled: true
    # -- The name override for the Guardrails PostgreSQL database.
    nameOverride: guardrailsdb
    image:
      repository: bitnamilegacy/postgresql
    volumePermissions:
      image:
        repository: bitnamilegacy/os-shell
    metrics:
      image:
        repository: bitnamilegacy/postgres-exporter
    # -- The architecture for the default PostgreSQL service.
    architecture: standalone
    serviceAccount:
      # -- The name of the service account for PostgreSQL.
      name: guardrails-postgresql
      # -- Specifies whether to create a new service account for PostgreSQL.
      create: true
    auth:
      # -- Whether to assign a password to the "postgres" admin user. If disabled, remote access is blocked for this user.
      enablePostgresUser: true
      # -- The user name to use for the PostgreSQL database.
      username: guardrails
      # -- The password for the PostgreSQL user.
      password: guardrails
      # -- The name for a custom database to create.
      database: nemo-guardrails
      # -- The name of an existing secret to use for PostgreSQL credentials.
      existingSecret: ""

  postgresWaitImage:
    # -- The repository location of the image used to wait for postgres to start.
    repository: "busybox"
    # -- The tag of the image used when waiting.
    tag: "latest"

  # -- External PostgreSQL configuration.
  externalDatabase:
    # -- The database host.
    host: ""
    # -- The database port number.
    port: ""
    # -- The username for the NeMo Guardrails external database.
    user: ""
    # -- The name of the database for the NeMo Guardrails service.
    database: ""
    # -- The name of an existing secret resource containing the database credentials.
    existingSecret: ""
    # -- The name of an existing secret key containing the database credentials.
    existingSecretPasswordKey: ""
    uriSecret:
      # -- The name of an existing secret that includes a full database URI.
      name: ""
      # -- The key within the existing secret that includes a full database URI.
      key: ""

  # -- Configuration for the "opentelemetry-collector" service.
  opentelemetry-collector:
    # -- Whether to enable the OpenTelemetry Collector service.
    enabled: false
    # -- The mode for the OpenTelemetry Collector service.
    mode: deployment
    # -- The configuration used by the OpenTelemetry Collector service.
    config:
      receivers:
        # -- The OTLP receiver for the OpenTelemetry Collector service.
        otlp:
          protocols:
            grpc: {}
            http:
              cors:
                allowed_origins:
                  - "*"
      exporters:
        # -- Debugging verbosity for the OpenTelemetry Collector service.
        debug:
          verbosity: detailed
      processors:
        # -- The batch processor for the OpenTelemetry Collector service.
        batch: {}
      service:
        pipelines:
          # -- The traces pipeline for the OpenTelemetry Collector service.
          traces:
            # -- The receivers for the traces pipeline for the OpenTelemetry Collector service.
            receivers: [otlp]
            # -- The exporters for the traces pipeline for the OpenTelemetry Collector service.
            exporters: [debug]
            # -- The processors for the traces pipeline for the OpenTelemetry Collector service.
            processors: [batch]
          # -- The metrics pipeline for the OpenTelemetry Collector service.
          metrics:
            # -- The receivers for the metrics pipeline for the OpenTelemetry Collector service.
            receivers: [otlp]
            # -- The exporters for the metrics pipeline for the OpenTelemetry Collector service.
            exporters: [debug]
            # -- The processors for the metrics pipeline for the OpenTelemetry Collector service.
            processors: [batch]
          # -- The logs pipeline for the OpenTelemetry Collector service.
          logs:
            # -- The receivers for the logs pipeline for the OpenTelemetry Collector service.
            receivers: [otlp]
            # -- The exporters for the logs pipeline for the OpenTelemetry Collector service.
            exporters: [debug]
            # -- The processors for the logs pipeline for the OpenTelemetry Collector service.
            processors: [batch]

  # -- Specifies labels to ensure that the NeMo Guardrails microservice is deployed only on certain nodes. To learn more, refer to the [Node Selector](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector) in the Kubernetes documentation.
  nodeSelector: {}

  # -- Specifies tolerations for pod assignment. To learn more, refer to the [Taint and Toleration](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) in the Kubernetes documentation.
  tolerations: []

  # -- Specifies affinity settings for the deployment. To learn more, refer to the [Affinity and Anti-Affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) in the Kubernetes documentation.
  affinity: {}
  # -- The service name for the NeMo Guardrails microservice.
  serviceName: nemo-guardrails

nemo-operator:
  # -- Whether to watch all namespaces, default is to restrict the NeMo Operator microservice to watch resources in the NMP deployment namespace.
  watchAllNamespaces: false

  # -- Additional environment variables to pass to controller manager. The format is `NAME: value` or `NAME: valueFrom: {object}`.
  env: {}

  controllerManager:
    serviceAccount:
      # -- Annotations to add to the service account.
      annotations: {}
    kubeRbacProxy:
      # -- Arguments to pass to the `kube-rbac-proxy` container.
      args:
        - --secure-listen-address=0.0.0.0:8443
        - --upstream=http://127.0.0.1:8080/
        - --logtostderr=true
        - --v=0
      containerSecurityContext:
        # -- Whether to allow container privilege escalation.
        allowPrivilegeEscalation: false
        capabilities:
          # -- List of capabilities to drop.
          drop:
            - ALL
      image:
        # -- The repository for the `kube-rbac-proxy` image.
        repository: gcr.io/kubebuilder/kube-rbac-proxy
        # -- The tag for the `kube-rbac-proxy` image.
        tag: v0.15.0
      resources:
        limits:
          # -- The CPU limit for the `kube-rbac-proxy` container.
          cpu: 500m
          # -- The memory limit for the `kube-rbac-proxy` container.
          memory: 128Mi
        requests:
          # -- The CPU request for the `kube-rbac-proxy` container.
          cpu: 5m
          # -- The memory request for the `kube-rbac-proxy` container.
          memory: 64Mi
    manager:
      # -- The scheduler to use for the controller manager.
      scheduler: volcano
      # -- Arguments to pass to the manager container.
      args:
        - --health-probe-bind-address=:8081
        - --metrics-bind-address=127.0.0.1:8080
        - --leader-elect
        - --leader-election-id=nemo.nko.nvidia.com
      containerSecurityContext:
        # -- Whether to allow container privilege escalation.
        allowPrivilegeEscalation: false
        capabilities:
          # -- List of capabilities to drop.
          drop:
            - ALL
      image:
        # -- The repository for the NeMo Operator microservice image.
        repository: nvcr.io/nvidia/nemo-microservices/nemo-operator
        # -- The tag for the NeMo Operator microservice image. The default value is `appVersion` from the `Chart.yaml` file.
        tag: ""
      resources:
        limits:
          # -- The CPU limit for the operator manager container.
          cpu: 1024m
          # -- The memory limit for the operator manager container.
          memory: 2Gi
        requests:
          # -- The CPU request for the operator manager container.
          cpu: 512m
          # -- The memory request for the operator manager container.
          memory: 1Gi
    # -- The number of operator replicas to run.
    replicas: 1

  # -- (list) Image pull secrets for accessing the NGC container registry.
  imagePullSecrets:
    - name: nvcrimagepullsecret
  # -- The Kubernetes cluster domain.
  kubernetesClusterDomain: cluster.local

  metricsService:
    # -- Whether to enable the metrics service for the NeMo Operator microservice. If you enable it, the microservice exposes a metrics endpoint for Prometheus.
    # Before installing this chart, you should have Prometheus installed in your environment.
    enabled: true
    # -- The metrics service ports configuration.
    ports:
      - name: https
        port: 8443
        protocol: TCP
        targetPort: https
    # -- The type of the metrics service.
    type: ClusterIP

nim-operator:
  nfd:
    nodeFeatureRules:
      # -- Specifies whether to enable device ID feature rules.
      deviceID: false

entity-store:
  # -- The number of NeMo Entity Store replicas to deploy.
  replicaCount: 1

  image:
    # -- The NeMo Entity Store image repository.
    repository: nvcr.io/nvidia/nemo-microservices/entity-store
    # -- The image pull policy to pull the NeMo Entity Store image
    pullPolicy: IfNotPresent
    # -- Specifies the image tag.
    tag: ""

  # -- List of image pull secrets. You can add multiple secrets to the list.
  imagePullSecrets:
    - name: nvcrimagepullsecret
  # -- String to partially override name on resulting Kubernetes objects when deployed.
  nameOverride: ""
  # -- String to fully override name on resulting Kubernetes objects when deployed.
  fullnameOverride: ""

  # -- Additional environment variables to pass to the NeMo Entity Store container. Format should be `NAME: value` or `NAME: valueFrom: {object}`.
  env: {}

  serviceAccount:
    # -- Whether to automatically mount the service account's API credentials.
    automount: true
    # -- Additional custom annotations for the service account.
    annotations: {}
    # -- The name of the service account to use.
    name: ""

  # -- Additional annotations for the NeMo Entity Store pods.
  podAnnotations: {}
  # -- Additional labels for the NeMo Entity Store pods.
  podLabels: {}

  # -- The pod security context for the NeMo Entity Store pods.
  podSecurityContext: {}

  # -- The security context for the NeMo Entity Store pods.
  securityContext: {}

  service:
    # -- The Kubernetes service type of the NeMo Entity Store microservice.
    type: ClusterIP
    # -- The service port for the NeMo Entity Store microservice.
    port: 8000

  openTelemetry:
    # -- Whether to enable OpenTelemetry integration.
    enabled: false

  livenessProbe:
    # -- The path for the liveness probe.
    httpGet:
      path: /health
      port: http
    # -- The initial delay seconds for the Kubernetes liveness probe.
    initialDelaySeconds: 3
    # -- The period seconds for the liveness probe.
    periodSeconds: 10
    # -- The timeout seconds for the liveness probe.
    timeoutSeconds: 20
    # -- The failure threshold for the liveness probe.
    failureThreshold: 10

  readinessProbe:
    # -- The path for the readiness probe.
    httpGet:
      path: /health
      port: http
    # -- The initial delay seconds for the readiness probe.
    initialDelaySeconds: 10
    # -- The period seconds for the readiness probe.
    periodSeconds: 10
    # -- The timeout seconds for the readiness probe.
    timeoutSeconds: 20
    # -- The failure threshold for the readiness probe.
    failureThreshold: 20

  # -- Requests and limits for underlying Kubernetes deployment for NeMo Entity Store.
  resources:
    {}
    # If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  autoscaling:
    # -- Whether to enable autoscaling.
    enabled: false
    # -- The minimum number of replicas.
    minReplicas: 1
    # -- The maximum number of replicas.
    maxReplicas: 100
    # -- The target CPU utilization percentage for autoscaling.
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  # -- Additional volumes for the deployment.
  volumes: []

  # -- Additional volume mounts for the deployment.
  volumeMounts: []

  # -- Additional NeMo Entity Store API server specific configs.
  appConfig:
    # -- The base URL for the NIM service used by the NeMo Entity Store microservice.
    BASE_URL_NIM: http://nemo-nim-proxy:8000
    # -- The base URL for the NeMo Data Store microservice.
    BASE_URL_DATASTORE: http://nemo-data-store:3000/v1/hf

  # -- Additional node selector configuration for the deployment.
  nodeSelector: {}

  # -- Additional tolerations for the deployment.
  tolerations: []

  # -- Additional affinity for the deployment.
  affinity: {}

  postgresql:
    # -- Whether to install the default PostgreSQL Helm chart. If enabled, the NeMo Entity Store microservice Helm chart uses the [PostgreSQL Helm chart from Bitnami](https://github.com/bitnami/charts/blob/main/bitnami/postgresql/values.yaml) to create a PostgreSQL database.
    enabled: true
    # -- The name override for the PostgreSQL database.
    nameOverride: entity-storedb
    image:
      repository: bitnamilegacy/postgresql
    volumePermissions:
      image:
        repository: bitnamilegacy/os-shell
    metrics:
      image:
        repository: bitnamilegacy/postgres-exporter
    serviceAccount:
      # -- The service account name for PostgreSQL.
      name: entity-store-postgresql
      # -- Specifies whether to create a new service account for PostgreSQL.
      create: true
    auth:
      # -- Whether to assign a password to the "postgres" admin user. If disabled, remote access is blocked for this user.
      enablePostgresUser: true
      # -- The user name to use for the PostgreSQL database.
      username: user
      # -- The password for the PostgreSQL user.
      password: pass
      # -- The name for a custom database to create.
      database: entity-store
      # -- The name of an existing secret to use for PostgreSQL credentials.
      existingSecret: ""
    # -- The PostgreSQL architecture. Available options are `standalone` or `replication`.
    architecture: standalone

  postgresWaitImage:
    # -- The repository location of the image used to wait for postgres to start.
    repository: "busybox"
    # -- The tag of the image used when waiting.
    tag: "latest"

  externalDatabase:
    # -- The database host.
    host: localhost
    # -- The database port number.
    port: 5432
    # -- The database username for the NeMo Entity Store service.
    user: user
    # -- The name of the database for the NeMo Entity Store service.
    database: entity-store
    # -- The name of an existing secret resource containing the database credentials.
    existingSecret: ""
    # -- The name of an existing secret key containing the database credentials.
    existingSecretPasswordKey: ""
    uriSecret:
      # -- The name of an existing secret that includes a full database URI.
      name: ""
      # -- The key within the existing secret that includes a full database URI.
      key: ""

  demo:
    # -- Whether to enable demo mode.
    enabled: false
    # -- The name of the NGC image pull secret for demo mode.
    ngcImagePullSecret: nvcrimagepullsecret
    # -- The NGC API key for demo mode.
    ngcApiKey: ""
  # -- The service name for the NeMo Entity Store microservice.
  serviceName: nemo-entity-store

volcano:
  # -- Specifies whether to enable the default Volcano scheduler installation. To learn more, see [Volcano](https://docs.nvidia.com/nemo/microservices/latest/set-up/deploy-as-microservices/customizer.html#volcano).
  enabled: false

deployment-management:
  # -- The number of replicas for the NeMo Deployment Management service.
  replicaCount: 1

  # -- Specifies a namespace to restrict the NeMo Deployment Management microservice to watch NIMs in. Leave it empty to watch all namespaces.
  nimNamespace: ""

  image:
    # -- The repository of the NeMo Deployment Management container image.
    repository: nvcr.io/nvidia/nemo-microservices/deployment-management
    # -- The container image pull policy for the NeMo Deployment Management container.
    pullPolicy: IfNotPresent
    # -- The container image tag. If not set, the default value is `appVersion` from the `Chart.yaml` file.
    tag: ""

  # -- (list) Image pull secrets for accessing the NGC container registry.
  imagePullSecrets:
    - name: nvcrimagepullsecret
  # -- String to partially override name on resulting Kubernetes objects when the NeMo Deployment Management microservice is deployed.
  nameOverride: ""
  # -- String to fully override the name on resulting Kubernetes objects when the NeMo Deployment Management microservice is deployed.
  fullnameOverride: ""

  # -- Additional environment variables to pass to the NeMo Deployment Management microservice container. The format is `NAME: value` or `NAME: valueFrom: {object}`.
  env: {}
  # -- Configures service account for RBAC for the NeMo Deployment Management microservice. Use the default setup, unless you understand what changes in RBAC settings you want to apply to the service.
  serviceAccount:
    # -- Whether to create a service account for the NeMo Deployment Management microservice. This is for setting RBAC up.
    create: true
    # -- Automatically mount a ServiceAccount's API credentials.
    automount: true
    # -- Annotations to add to the service account.
    annotations: {}
    # -- The name of the service account to use. If not set and create is true, a name is generated using the fullname template.
    name: ""

  # -- Pod annotations.
  podAnnotations: {}
  # -- Pod labels.
  podLabels: {}

  # -- Pod security context settings. Use the default settings, unless you understand what changes in the pod security context settings you want to apply.
  podSecurityContext:
    {}
    # fsGroup: 2000

  # -- Security context settings. Use the default settings, unless you understand what changes in the security context settings you want to apply.
  securityContext:
    # -- Enable read-only root filesystem. You can also add any values for [Kubernetes security context](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.32/#securitycontext-v1-core) in this field.
    readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  service:
    deploymentPort:
      # -- The service type of the deployment port.
      type: ClusterIP
      # -- The port number for the service.
      port: 8000

  # -- Configures Kubernetes resource requests and limits for the NeMo Deployment Management microservice. Use the default settings and leave it with the empty object as is, unless you understand what changes you want to make.
  resources:
    {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  livenessProbe:
    httpGet:
      # -- The HTTP path for the Kubernetes liveness probe. Do not change this.
      path: /health
      # -- The port name for the Kubernetes liveness probe. Do not change this.
      port: http
  readinessProbe:
    httpGet:
      # -- The HTTP path for the Kubernetes readiness probe. Do not change this.
      path: /health
      # -- The port name for the Kubernetes readiness probe. Do not change this.
      port: http

  autoscaling:
    # -- Whether to enable autoscaling for the NeMo Deployment Management microservice.
    enabled: false
    # -- The minimum number of replicas.
    minReplicas: 1
    # -- The maximum number of replicas.
    maxReplicas: 100
    # -- The target CPU utilization percentage.
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  # -- Additional volumes for the NeMo Deployment Management microservice. Use the default settings and leave it with the empty list as is, unless you understand what changes you want to make.
  volumes: []

  # -- Additional volume mounts for the NeMo Deployment Management microservice. Use the default settings and leave it with the empty list as is, unless you understand what changes you want to make.
  volumeMounts: []

  # -- Specifies labels to ensure that the microservice is deployed only on certain nodes. To learn more, refer to the [Node Selector](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector) in the Kubernetes documentation.
  nodeSelector: {}

  # -- Specifies tolerations for pod assignment. To learn more, refer to the [Taint and Toleration](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) in the Kubernetes documentation.
  tolerations: []

  # -- Specifies affinity settings for the deployment. To learn more, refer to the [Affinity and Anti-Affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) in the Kubernetes documentation.
  affinity: {}

  # -- Prometheus metrics collection configuration for monitoring the NeMo Deployment Management microservice. To enable this, you should install the Prometheus operator before deploying the NeMo Deployment Management microservice.
  monitoring:
    # -- Whether to enable monitoring for the NeMo Deployment Management microservice.
    enabled: false

  dataStore:
    # -- The URL for the NeMo Data Store service.
    url: http://nemo-data-store:3000
    secret:
      # -- Whether to create a secret for the data store huggingface token. If false, the secret must be created manually.
      create: true
      # -- The name of the secret that will be created. If create is false, a secret with this name must be created manually and have the HF_TOKEN key set.
      name: "nemo-deployment-management-service-ds-hf-token"

  # -- Properties to configure NIM deployments for the NeMo Deployment Management microservice.
  deployments:
    # -- The default storage class for NIM deployments.
    # -- Storage classes utilizing NFSv3+ may encounter issues with huggingface-cli file locking.
    defaultStorageClass: ""
    # -- The URL for PEFT model sources (typically points to NeMo Entity Store)
    nimPeftSource: http://nemo-entity-store:8000
    # -- The URL for the NeMo Entity Store service.
    entityStoreUrl: http://nemo-entity-store:8000
    # -- The period in seconds for model synchronization.
    modelSyncPeriod: "30"
    metrics:
      # -- Whether to enable metrics collection for the NIM deployments.
      enabled: false
    # -- The size of the PVC for the NIM deployments.
    nimPvcSize: 200Gi
    # -- The image to use for pulling models from NeMo Data Store. Must have the huggingface-cli binary installed.
    modelPullerImage: nvcr.io/nvidia/nemo-microservices/nds-v2-huggingface-cli:25.06
    # -- The pull secret used to pull the model puller image.
    modelPullerPullSecret: nvcrimagepullsecret
    autoscaling:
      # -- Whether to enable autoscaling for the NIM deployments.
      enabled: false
      # -- Autoscaling specification for the NIM deployments.
      spec:
        maxReplicas: 5
        metrics:
          - pods:
              metric:
                name: gpu_cache_usage_perc
              target:
                averageValue: 750m
                type: AverageValue
            type: Pods
        minReplicas: 1
    # -- List of image pull secrets for the NIM deployments.
    nimImagePullSecrets:
      - nvcrimagepullsecret
  # -- The service name for the NeMo Deployment Management microservice.
  serviceName: nemo-deployment-management

nim-proxy:
  # -- The service name for the NIM Proxy microservice.
  serviceName: nemo-nim-proxy

  # -- The number of replicas for the NeMo NIM Proxy service.
  replicaCount: 1

  # -- Specifies a namespace to restrict the NIM Proxy microservice to watch NIMs in. Leave it empty to watch all namespaces.
  nimNamespace: ""

  # -- Additional environment variables to pass to containers. The format is `NAME: value` or `NAME: valueFrom: {object}`.
  env: {}

  image:
    # -- The repository of the NIM Proxy container image.
    repository: nvcr.io/nvidia/nemo-microservices/nim-proxy
    # -- The container image pull policy for the NIM Proxy container.
    pullPolicy: IfNotPresent
    # -- The container image tag. If not set, the default value is `appVersion` from the `Chart.yaml` file.
    tag: ""

  # -- (list) Image pull secrets for accessing the NGC container registry.
  imagePullSecrets: []
  # -- String to partially override name on resulting Kubernetes objects when the NIM Proxy microservice is deployed.
  nameOverride: ""
  # -- String to fully override the name on resulting Kubernetes objects when the NIM Proxy microservice is deployed.
  fullnameOverride: ""

  serviceAccount:
    # -- Whether to create a service account for the NIM Proxy microservice.
    create: true
    # -- Whether to automatically mount a ServiceAccount's API credentials.
    automount: true
    # -- Annotations to add to the service account.
    annotations: {}
    # -- The name of the service account to use. If not set and create is `true`, a name is generated using the full name template.
    name: ""
  # -- Pod annotations.
  podAnnotations: {}
  # -- Pod labels.
  podLabels: {}
  # -- Pod security context. Use the default settings, unless you understand what changes in the pod security context settings you want to apply.
  podSecurityContext:
    {}
    # fsGroup: 2000

  # -- Security context. Use the default settings, unless you understand what changes in the security context settings you want to apply.
  securityContext:
    # capabilities:
    #   drop:
    #   - ALL
    # -- Whether to run with a read-only root filesystem.
    readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  service:
    main:
      # -- The type of the main service.
      type: ClusterIP
      # -- The port of the main service.
      port: 8000
    metricsPort:
      # -- The type of the metrics service.
      type: ClusterIP
      # -- The port of the metrics service.
      port: 8001

  # -- Configures Kubernetes resource requests and limits for the NIM Proxy microservice. Use the default settings and leave it with the empty object as is, unless you understand what changes you want to make.
  resources:
    {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  livenessProbe:
    httpGet:
      # -- The HTTP path for the Kubernetes liveness probe. Do not change this.
      path: /health
      # -- The port name for the Kubernetes liveness probe. Do not change this.
      port: http

  readinessProbe:
    httpGet:
      # -- The HTTP path for the Kubernetes readiness probe. Do not change this.
      path: /health
      # -- The port name for the Kubernetes readiness probe. Do not change this.
      port: http

  autoscaling:
    # -- Whether to enable horizontal pod autoscaling for the NIM Proxy microservice.
    enabled: false
    # -- The minimum number of replicas for the NIM Proxy microservice.
    minReplicas: 1
    # -- The maximum number of replicas for the NIM Proxy microservice.
    maxReplicas: 100
    # -- The target CPU utilization percentage for the NIM Proxy microservice.
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  # -- Additional volumes for the NIM Proxy microservice. Use the default settings and leave it with the empty list as is, unless you understand what changes you want to make.
  volumes: []
  # - name: foo
  #   secret:
  #     secretName: mysecret
  #     optional: false

  # -- Additional volume mounts for the NIM Proxy microservice. Use the default settings and leave it with the empty list as is, unless you understand what changes you want to make.
  volumeMounts: []
  # - name: foo
  #   mountPath: "/etc/foo"
  #   readOnly: true

  # -- Specifies labels to ensure that the microservice is deployed only on certain nodes. To learn more, refer to [Node Selector](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector) in the Kubernetes documentation.
  nodeSelector: {}

  # -- Specifies tolerations for pod assignment. To learn more, refer to [Taint and Toleration](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) in the Kubernetes documentation.
  tolerations: []

  # -- Specifies affinity settings for the deployment. To learn more, refer to [Affinity and Anti-Affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) in the Kubernetes documentation.
  affinity: {}

  # -- Prometheus metrics collection configuration for monitoring the NIM Proxy microservice. To enable this, you should install the Prometheus operator before deploying the NIM Proxy microservice.
  monitoring:
    # -- Whether to enable monitoring for the NIM Proxy microservice.
    enabled: false
    # -- The scrape interval for monitoring.
    interval: 30s
    # -- The metrics path for monitoring.
    path: /v1/metrics
    # -- The scheme for monitoring.
    scheme: http

  # -- Pod Disruption Budget configuration for the NIM Proxy microservice. This helps maintain high availability during voluntary disruptions like node drains or upgrades. To learn more, refer to [Pod Disruption Budget](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/) in the Kubernetes documentation.
  podDisruptionBudget:
    # -- Whether to enable the Pod Disruption Budget for the NIM Proxy microservice.
    enabled: false
    # -- The minimum number of pods that must be available during a disruption. Cannot be used simultaneously with maxUnavailable.
    minAvailable: ""
    # -- The maximum number of pods that can be unavailable during a disruption. Cannot be used simultaneously with minAvailable.
    maxUnavailable: ""

studio:
  # -- The service name for the NeMo Studio UI microservice.
  serviceName: nemo-studio
  replicaCount: 1
  # -- Specifies a base url or path (e.g. studio for https://domain.com/studio)
  baseRoute: studio
  # -- The base url for platform endpoints
  platformBaseUrl: http://nemo.test:3000
  # -- The internal URL of the NeMo NIM Proxy microservice.
  nimProxyInternalUrl: http://nemo-nim-proxy:8000
  # -- The external URL of the NeMo NIM Proxy microservice.
  nimProxyUrl: http://nim.test:3000
  # -- The external URL of the NeMo Data Store microservice.
  dataStoreUrl: http://data-store.test:3000
  # -- The external URL of the NeMo Customizer microservice (if different from the platform base url).
  customizerUrl: ""
  # -- The external URL of the NeMo Evaluator microservice (if different from the platform base url).
  evaluatorUrl: ""
  # -- The external URL of the NeMo Entity Store microservice (if different from the platform base url).
  entityStoreUrl: ""
  # -- OpenTelemetry environment configuration variables for the Studio UI
  openTelemetry:
    enabled: false
    serviceName: nemo-studio-ui
    proxyURL: ""
    collectorURL: ""

  # Environment variables to pass to containers. This is an object formatted like NAME: value or NAME: valueFrom: {object}
  env: {}

  image:
    repository: nvcr.io/nvidia/nemo-microservices/nemo-studio-ui
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: ""

  imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: ""

  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Automatically mount a ServiceAccount's API credentials?
    automount: true
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""

  podAnnotations: {}
  podLabels: {}

  podSecurityContext:
    {}
    # fsGroup: 2000

  securityContext:
    # capabilities:
    #   drop:
    #   - ALL
    readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  service:
    type: ClusterIP
    port: 3000

  resources:
    {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  livenessProbe:
    httpGet:
      path: /health
      port: http
  readinessProbe:
    httpGet:
      path: /ready
      port: http

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  # This enables doing a runtime replacement of env variables for Vite.
  volumes:
    - name: html
      emptyDir: {}
    - name: run
      emptyDir: {}
    - name: temp
      emptyDir: {}
  volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
    - name: run
      mountPath: /var/run
    - name: temp
      mountPath: /tmp

  nodeSelector: {}

  tolerations: []

  affinity: {}

ingress:
  # -- Specifies whether to enable the ingress.
  enabled: false
  # -- Annotations for the ingress resource.
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: 50g
    ingress.kubernetes.io/proxy-body-size: 50g
  # -- The ingress class to use if your cluster has more than one class.
  className: ""
  # -- TLS configurations.
  tls: []
  # -- A map of hosts and their corresponding paths for the ingress.
  hosts:
    default:
      # -- The host name for the default ingress host.
      name: ""
      # -- The path rules for the default ingress host.
      paths:
        - path: /v1/namespaces
          pathType: Prefix
          service: nemo-entity-store
          port: 8000
        - path: /v1/projects
          pathType: Prefix
          service: nemo-entity-store
          port: 8000
        - path: /v1/datasets
          pathType: Prefix
          service: nemo-entity-store
          port: 8000
        - path: /v1/repos
          pathType: Prefix
          service: nemo-entity-store
          port: 8000
        - path: /v1/models
          pathType: Prefix
          service: nemo-entity-store
          port: 8000
        - path: /v1/customization
          pathType: Prefix
          service: nemo-customizer
          port: 8000
        - path: /v1/evaluation
          pathType: Prefix
          service: nemo-evaluator
          port: 7331
        - path: /v2/evaluation
          pathType: Prefix
          service: nemo-evaluator
          port: 7331
        - path: /v1/guardrail
          pathType: Prefix
          service: nemo-guardrails
          port: 7331
        - path: /v1/deployment
          pathType: Prefix
          service: nemo-deployment-management
          port: 8000
        - path: /v1/data-designer
          pathType: Prefix
          service: nemo-data-designer
          port: 8000
        - path: /v1beta1/audit
          pathType: Prefix
          service: nemo-auditor
          port: 5000
        - path: /v1beta1/safe-synthesizer
          pathType: Prefix
          service: nemo-safe-synthesizer
          port: 8000
        - path: /v1/jobs
          pathType: Prefix
          service: nemo-core-api
          port: 8000
        - path: /v2/inference/gateway # Inference gateway
          pathType: Prefix
          service: nemo-core-api
          port: 8000
        - path: /v2/inference # Models service
          pathType: Prefix
          service: nemo-core-api
          port: 8000
        - path: /v2/models # Models service
          pathType: Prefix
          service: nemo-core-api
          port: 8000
        - path: /v1/intake
          pathType: Prefix
          service: nemo-intake
          port: 8000        
        - path: /studio
          pathType: Prefix
          service: nemo-studio
          port: 3000
    nimProxy:
      # -- The host name for the second ingress host for the NIM Proxy microservice.
      name: nim.test
      # -- The path rules for the second ingress host.
      paths:
        - path: /v1/completions
          pathType: Prefix
          service: nemo-nim-proxy
          port: 8000
        - path: /v1/chat
          pathType: Prefix
          service: nemo-nim-proxy
          port: 8000
        - path: /v1/embeddings
          pathType: Prefix
          service: nemo-nim-proxy
          port: 8000
        - path: /v1/classify
          pathType: Prefix
          service: nemo-nim-proxy
          port: 8000
        - path: /v1/models
          pathType: Prefix
          service: nemo-nim-proxy
          port: 8000
    dataStore:
      # -- The host name for the third ingress host for the NeMo Data Store microservice.
      name: data-store.test
      # -- The path rules for the third ingress host.
      paths:
        - path: /
          pathType: Prefix
          service: nemo-data-store
          port: 3000

# -- Specifies whether to enable the virtual service. If you are not using istio and virtualservices, it can be useful to create some virtual services for the NeMo Microservices system. Don't enable unless you use istio.
# @default -- A virtual service configuration template.
virtualService:
  # -- Specifies whether to enable the virtual service.
  enabled: true
  # -- Labels for the virtual service.
  labels: {}
  # -- Annotations for the virtual service.
  annotations: {}
  main:
    # -- A list of gateways for the virtual service.
    gateways:
      - "istio-system/ezaf-gateway"
    # -- A list of hosts for the virtual service.
    hosts:
      - "nemo-microservices.${DOMAIN_NAME}"
    entries:
      customizer:
        # -- The CORS policy for the virtual NeMo Customizer service.
        corsPolicy: {}
        # -- The match for the virtual NeMo Customizer service.
        match:
          - uri:
              prefix: "/v1/customization/"
        # -- The route for the virtual NeMo Customizer service.
        rewrite:
          uri: /
        # -- The route for the virtual NeMo Customizer service.
        route:
          - destination:
              host: nemo-customizer
              port:
                number: 8000
      deployment-management:
        # -- The CORS policy for the virtual NeMo Deployment Management service.
        corsPolicy: {}
        # -- The match for the virtual NeMo Deployment Management service.
        match:
          - uri:
              prefix: "/v1/deployment/"
        # -- The route for the virtual NeMo Deployment Management service.
        rewrite:
          uri: /
        # -- The route for the virtual NeMo Deployment Management service.
        route:
          - destination:
              host: nemo-deployment-management
              port:
                number: 8000
      entity-store:
        # -- The CORS policy for the virtual NeMo Entity Store service.
        corsPolicy: {}
        # -- The match for the virtual NeMo Entity Store service.
        match:
          - uri:
              prefix: "/v1/namespaces/"
          - uri:
              prefix: "/v1/projects/"
          - uri:
              prefix: "/v1/datasets/"
          - uri:
              prefix: "/v1/repos/"
          - uri:
              prefix: "/v1/models/"
        # -- The route for the virtual NeMo Entity Store service.
        rewrite:
          uri: /
        # -- The route for the virtual NeMo Entity Store service.
        route:
          - destination:
              host: nemo-entity-store
              port:
                number: 8000
      evaluator:
        # -- The CORS policy for the virtual NeMo Evaluator service.
        corsPolicy: {}
        # -- The match for the virtual NeMo Evaluator service.
        match:
          - uri:
              prefix: "/v1/evaluation/"
          - uri:
              prefix: "/v2/evaluation/"
        # -- The route for the virtual NeMo Evaluator service.
        rewrite:
          uri: /
        # -- The route for the virtual NeMo Evaluator service.
        route:
          - destination:
              host: nemo-evaluator
              port:
                number: 7331
      guardrails:
        # -- The CORS policy for the virtual NeMo Guardrails service.
        corsPolicy: {}
        # -- The match for the virtual NeMo Guardrails service.
        match:
          - uri:
              prefix: "/"
        # -- The route for the virtual NeMo Guardrails service.
        rewrite:
          uri: /
        # -- The route for the virtual NeMo Guardrails service.
        route:
          - destination:
              host: nemo-guardrails
              port:
                number: 7331
      auditor:
        # -- The CORS policy for the virtual NeMo Auditor service.
        corsPolicy: {}
        # -- The match for the virtual NeMo Auditor service.
        match:
          - uri:
              prefix: /v1beta1/audit/
        # -- The route for the virtual NeMo Auditor service.
        rewrite:
          uri: /
        # -- The route for the virtual NeMo Auditor service.
        route:
          - destination:
              host: nemo-auditor
              port:
                number: 5000
      safe-synthesizer:
        # -- The CORS policy for the virtual NeMo Safe Synthesizer service.
        corsPolicy: {}
        # -- The match for the virtual NeMo Safe Synthesizer service.
        match:
          - uri:
              prefix: /v1beta1/safe-synthesizer/
        # -- The route for the virtual NeMo Safe Synthesizer service.
        rewrite:
          uri: /
        # -- The route for the virtual NeMo Safe Synthesizer service.
        route:
          - destination:
              host: nemo-safe-synthesizer
              port:
                number: 8000
      core:
        # -- The CORS policy for the virtual NeMo Core service.
        corsPolicy: {}
        # -- The match for the virtual NeMo Core service.
        match:
          - uri:
              prefix: /v1/jobs/
          - uri:
              prefix: /v2/inference/gateway/ # Inference gatewa/y
          - uri:
              prefix: /v2/inference/ # Models servic/e
          - uri:
              prefix: /v2/models/ # Models servic/e
        # -- The route for the virtual NeMo Core service.
        rewrite:
          uri: /
        # -- The route for the virtual NeMo Core service.
        route:
          - destination:
              host: nemo-core-api
              port:
                number: 8000
      intake:
        # -- The CORS policy for the virtual NeMo Intake service.
        corsPolicy: {}
        # -- The match for the virtual NeMo Intake service.
        match:
          - uri:
              prefix: /v1/intake/
        # -- The route for the virtual NeMo Intake service.
        rewrite:
          uri: /
        # -- The route for the virtual NeMo Intake service.
        route:
          - destination:
              host: nemo-intake
              port:
                number: 8000
  additional:
    # -- Additional virtual service configurations.
    data-store:
      # -- The gateways for the virtual NeMo Data Store service.
      gateways: []
      # -- The hosts for the virtual NeMo Data Store service.
      hosts: []
      # -- The entries for the virtual NeMo Data Store service.
      entries:
        data-store:
          # -- The CORS policy for the virtual NeMo Data Store service.
          corsPolicy: {}
          # -- The match for the virtual NeMo Data Store service.
          match:
            - uri:
                prefix: /
          # -- The route for the virtual NeMo Data Store service.
          rewrite:
          uri: /
          # -- The route for the virtual NeMo Data Store service.
          route:
            - destination:
                host: nemo-data-store
                port:
                  number: 3000
    nim-proxy:
      # -- The gateways for the virtual NIM Proxy service.
      gateways:
        - "istio-system/ezaf-gateway"
      # -- The hosts for the virtual NIM Proxy service.
      hosts:
        - "nemo-nim-proxy.${DOMAIN_NAME}"
      entries:
        nim-proxy:
          # -- The CORS policy for the virtual NIM Proxy service.
          corsPolicy: {}
          # -- The match for the virtual NIM Proxy service.
          match:
            - uri:
                prefix: /
          # -- The route for the virtual NIM Proxy service.
          rewrite:
          uri: /
          # -- The route for the virtual NIM Proxy service.
          route:
            - destination:
                host: nemo-nim-proxy
                port:
                  number: 8000

data-designer:
  # -- The service name for the NeMo Data Designer microservice.
  serviceName: nemo-data-designer

  # -- Data Designer application settings
  config:
    port: 8000
    log_level: "INFO"

    # -- The configuration for supported seed dataset sources
    seed_dataset_source_registry:
      sources:
        - endpoint: http://nemo-data-store:3000/v1/hf

    preview_num_records:
      # -- The default number of records to return when generating a preview dataset
      default: 10
      # -- The maximum number of records to allow when generating a preview dataset
      max: 10

    # -- The configuration for supported model providers
    model_provider_registry:
      default: "nimproxy"
      providers:
        - name: "nimproxy"
          endpoint: "http://nemo-nim-proxy:8000/v1"

    # -- A list of default model configs available in all dataset generation requests
    default_model_configs: []

  # -- Number of replicas for the NeMo Data Designer deployment.
  replicaCount: 1

  image:
    # -- The repository location of the NeMo Data Designer container image.
    repository: nvcr.io/nvidia/nemo-microservices/data-designer
    # -- The image pull policy for the NeMo Data Designer container image.
    pullPolicy: IfNotPresent
    # -- The tag of the NeMo Data Designer container image.
    tag: ""

  # -- Specifies the list of secret names that are needed for the main container and any init containers.
  imagePullSecrets: []

  # -- Overrides the chart name.
  nameOverride: ""

  # -- Overrides the full chart name.
  fullnameOverride: ""

  serviceAccount:
    # -- Whether to create a service account for the NeMo Data Designer microservice.
    create: true
    # -- Whether to automatically mount the service account token.
    automount: true
    # -- Annotations to be added to the service account.
    annotations: {}
    # -- The name of the service account to use.
    name: ""

  # -- Specifies additional annotations to the main deployment pods.
  podAnnotations: {}

  # -- Specifies additional labels to the main deployment pods.
  podLabels: {}

  # -- Specifies privilege and access control settings for the pod.
  podSecurityContext:
    # -- Specifies the file system owner group id.
    fsGroup: 1000

  # -- Specifies security context for the container.
  securityContext:
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  # -- Specifies the service type and the port for the deployment.
  service:
    type: ClusterIP
    port: 8000

  # -- Resource requests and limits.
  resources:
    {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 500m
    #   memory: 1Gi
    # requests:
    #   cpu: 250m
    #   memory: 512Mi

  # -- Liveness probe configuration.
  livenessProbe:
    httpGet:
      path: /health
      port: http
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3

  # -- Readiness probe configuration.
  readinessProbe:
    httpGet:
      path: /health
      port: http
    initialDelaySeconds: 5
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 3

  # -- Autoscaling configuration.
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  # -- Environment variables for the Data Designer container.
  env: {}

  # -- Additional volumes for the deployment.
  volumes: []

  # -- Additional volume mounts for the deployment.
  volumeMounts: []

  # -- Additional node selector configuration for the deployment.
  nodeSelector: {}

  # -- Additional tolerations for the deployment.
  tolerations: []

  # -- Affinity configuration for the deployment.
  affinity: {}

core:
  # -- Sets predictable name for the NeMo Core service
  serviceName: nemo-core

  # Config is the configuration for the core services
  config:
    # -- platform is the global platform configuration which is shared across all services
    platform:
      # -- base_url is the base URL for the platform. If not set, it will default to the core service URL
      base_url: ""

      # -- enable_service_account_auth enables service account token authentication in-between services
      enable_service_account_auth: true

    # -- jobs is the configuration specific to executing jobs on the platform
    jobs:
      # -- port is the port the jobs service will listen on
      port: 8000

      # -- storage config for the persistent volume claim that is shared by jobs on the platform
      storage:
        # -- If set, pods will mount this persistent volume for job-scoped storage
        # and we will not create a new persistent volume claim.
        existingPersistentVolumeName: ""

        # -- Which storageClass to use when creating a new persistent volume claim.
        # Leaving as empty string will use the cluster's default storageClass.
        storageClass: ""

        # -- accessModes for the persistent volume claim. This should include `ReadWriteMany` to ensure
        # multiple job pods can write to the volume concurrently.
        accessModes:
          - ReadWriteMany

        # -- size of the persistent volume claim used for jobs storage
        size: 200Gi

        # -- volumePermissionsImage is the image used to set permissions on the volume
        volumePermissionsImage: "busybox"

      # -- schedule_interval_seconds is how frequently to schedule created jobs
      schedule_interval_seconds: 10

      # -- reconcile_interval_seconds is how frequently to reconcile jobs
      reconcile_interval_seconds: 5

      # -- ttl_seconds_after_finished is the time to live in seconds for finished jobs before they are cleaned up
      ttl_seconds_after_finished: 10800

      # -- enabled_backends is the configuration for enabling job execution backends.
      # On Kubernetes, kubernetes_job is always enabled, while other backends can be optionally enabled in this section.
      enabled_backends:
        # -- Enable Volcano jobs backend
        volcano: false

      # Executor profiles configuration
      executors:
        - profile: default
          backend: kubernetes_job
          provider: cpu
        - profile: default
          backend: kubernetes_job
          provider: gpu

        ## You can add custom profiles here to match your cluster setup.
        #  For example, you can use node selectors, tolerations, and affinity to
        #  target specific nodes in your cluster.
        #- profile: custom
        #  backend: kubernetes_job
        #  provider: gpu
        #  config:
        #    pod_metadata:
        #      labels:
        #        sidecar.istio.io/inject: "false"
        #      annotations:
        #        example.com/annotation: "value"
        #.   job_metadata:
        #      labels:
        #        my-custom-label: "value"
        #      annotations:
        #        example.com/annotation: "value"
        #    node_selector:
        #      kubernetes.io/arch: amd64
        #    tolerations:
        #      - key: nvidia.com/gpu
        #        operator: Exists
        #        effect: NoSchedule
        #    affinity:
        #      nodeAffinity:
        #        requiredDuringSchedulingIgnoredDuringExecution:
        #          nodeSelectorTerms:
        #            - matchExpressions:
        #                - key: highmem
        #                  operator: In
        #                  values:
        #                    - "true"
        #    # Disable logging sidecar to avoid security policy violations with hostPath volumes
        #    logging:
        #      enabled: false
        ## You can also provide profiles for other backends that are supported.
        #  - profile: custom
        #    backend: kubernetes_job
        #    provider: cpu
        #    config:
        #      custom_config_key: custom_config_value

    # -- models is the configuration specific to model management on the platform
    models:
      # -- host is the host the models service will listen on
      host: "0.0.0.0"

      # -- port is the port the models service will listen on
      port: 8000

      # -- HuggingFace model puller image for weights in data store or huggingface
      huggingface_model_puller:
        # -- Registry for the HuggingFace model puller image
        registry: "nvcr.io"
        # -- Repository for the HuggingFace model puller image
        repository: "nvidia/nemo-microservices/nds-v2-huggingface-cli"
        # -- Tag for the HuggingFace model puller image (defaults to Chart.appVersion)
        tag: ""

      # -- DataStore authentication secret configuration
      datastore_secret:
        # -- Whether to create the datastore secret for api authentication (HF_TOKEN)
        create: true
        # -- The name of the secret to be created
        name: "nemo-models-datastore-token"

      # -- override database configuration for the models service
      database:
        # -- Database name override (if empty, use value from externalDatabase or postgresql, whichever is set)
        name: "entity-store"
        # -- Database host override (if empty, use value from externalDatabase or postgresql, whichever is set)
        host: "nemo-entity-storedb"
        # -- Database port override (if empty, use value from externalDatabase or postgresql, whichever is set)
        port: 5432
        # -- Database user override (if empty, use value from externalDatabase or postgresql, whichever is set)
        user: "user"
        # -- Database password override (if empty, use value from externalDatabase or postgresql, whichever is set)
        # -- Note: This will be stored in a Kubernetes secret. Mutually exclusive with passwordExistingSecret
        password: "pass"
        # -- Use an existing secret for the database password (mutually exclusive with password)
        passwordExistingSecret:
          # -- Name of the existing secret containing the password
          name: ""
          # -- Key in the secret that contains the password
          key: ""

      # -- controller configuration for the models service
      controller:
        # -- interval in seconds for the models controller to reconcile deployments
        interval_seconds: 10

        # -- model_deployment_garbage_collection_ttl_seconds is the time-to-live in seconds for DELETED deployments
        # before they are permanently removed from the database
        model_deployment_garbage_collection_ttl_seconds: 30

        # -- backends configuration for the models controller
        # The backends define how model deployments are executed
        backends:
          k8s-nim-operator:
            enabled: true
            config:
              # -- Default storage class for PVCs created by NIM deployments
              default_storage_class: ""
              # -- Default PVC size for model storage (used if not specified in deployment config)
              default_pvc_size: "200Gi"
              # -- LoRA/PEFT source endpoint (only used when lora_enabled is true)
              peft_source: "http://nemo-entity-store:8000"
              # -- PEFT refresh interval in seconds (only used when lora_enabled is true)
              peft_refresh_interval: 30
              # -- Default user ID for NIM containers (security context, optional)
              default_user_id: null
              # -- Default group ID for NIM containers (security context, optional)
              default_group_id: null
              # -- Existing Kubernetes secret name for DataStore authentication (HF_TOKEN)
              datastore_auth_secret: "nemo-models-datastore-token"
              # -- The name of the image pull secret for the modelPuller image
              huggingface_model_puller_image_pull_secret: "nvcrimagepullsecret"
              # -- NGC API key secret name for pulling NIM images
              auth_secret: "ngc-api"
              # -- Default NIMService image repository (used if not specified in deployment config)
              default_nimservice_image: "nvcr.io/nim/nvidia/llm-nim"
              # -- Default NIMService image tag (used if not specified in deployment config)
              default_nimservice_image_tag: "1.14.1"
              # -- Default guided decoding backend for NIM (e.g., 'outlines', 'auto', 'lm-format-enforcer')
              nim_guided_decoding_backend: "outlines"
              # -- Kubernetes namespace for NIM deployments (defaults to controller's namespace if not set)
              namespace: ""
              # -- Default Kubernetes resource requirements for all NIM deployments (optional)
              # default_resources:
              #   requests:
              #     cpu: "2"
              #     memory: "8Gi"
              #   limits:
              #     memory: "16Gi"
              default_resources: null
              # -- Default Kubernetes tolerations for all NIM deployments (optional)
              # default_tolerations:
              #   - key: "nvidia.com/gpu"
              #     operator: "Exists"
              #     effect: "NoSchedule"
              default_tolerations: null
              # -- Default Kubernetes node selector for all NIM deployments (optional)
              # default_node_selector:
              #   node-type: "gpu-node"
              #   zone: "us-west1-a"
              default_node_selector: null
              # -- Default grace period in seconds for NIM startup probe (optional, defaults to 600)
              # Set higher for large models that take longer to load (e.g., 1200 for 20 minutes)
              default_startup_probe_grace_period_seconds: 600

    # -- inference_gateway is the configuration specific to inference request routing
    inference_gateway:
      # -- port is the port the inference gateway service will listen on
      port: 8000

      # -- host is the host the inference gateway service will listen on
      host: "0.0.0.0"

      # -- refresh_model_cache_interval_sec is how frequently to refresh the internal model cache
      refresh_model_cache_interval_sec: 3

  # -- Container image configuration for the NeMo Core microservice.
  # @default -- This object has the following default values for the image configuration.
  image:
    # -- The registry where the NeMo Core image is located.
    repository: nvcr.io/nvidia/nemo-microservices/nmp-core
    # -- The image pull policy determining when to pull new images.
    pullPolicy: IfNotPresent
    # -- The image tag to use.
    tag: ""

  # -- API service configuration for the NeMo Core microservice.
  # @default -- This object has the following default values for the API configuration.
  api:
    # -- Number of replicas for the API service.
    replicaCount: 1

    # -- Additional arguments to pass to the Core API service
    extraArgs: []
      # - "--target=jobs-api"

    # -- Service account configuration for the API service.
    # @default -- This object has the following default values for the service account configuration.
    serviceAccount:
      # -- Specifies whether a service account should be created.
      create: true
      # -- Automatically mount a ServiceAccount's API credentials.
      automount: true
      # -- Annotations to add to the service account.
      annotations: {}
      # -- The name of the service account to use. If not set and create is true, a name is generated using the fullname template.
      name: ""

    # -- Annotations to add to the API service deployment.
    annotations: {}

    # -- Annotations to add to the API service pod.
    podAnnotations: {}
    # -- Labels for the API service pod.
    podLabels: {}

    # -- Pod-level security context settings for the API service.
    # @default -- This object has the following default values for the pod security context.
    podSecurityContext:
      # -- The file system group ID to use for all containers.
      fsGroup: 1000

    # -- Container-level security context settings for the API service.
    securityContext: {}

    # -- Service configuration for the API service.
    # @default -- This object has the following default values for the service configuration.
    service:
      # -- The Kubernetes service type to create.
      type: ClusterIP
      # -- The port number to expose for the service.
      port: 8000

    resources:
      {}
      # We usually recommend not to specify default resources and to leave this as a conscious
      # choice for the user. This also increases chances charts run on environments with little
      # resources, such as Minikube. If you do want to specify resources, uncomment the following
      # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
      # limits:
      #   cpu: 100m
      #   memory: 128Mi
      # requests:
      #   cpu: 100m
      #   memory: 128Mi

    # -- Liveness probe configuration for the api service.
    # @default -- This object has the following default values for the liveness probe configuration.
    livenessProbe:
      # -- The HTTP GET request to use for the readiness probe.
      httpGet:
        path: /health
        port: http
      # -- The frequency in seconds to perform the readiness probe.
      periodSeconds: 10
      # -- The timeout in seconds for the readiness probe.
      timeoutSeconds: 5
      # -- The failure threshold for the readiness probe.
      failureThreshold: 3

    # -- Readiness probe configuration for the api service.
    # @default -- This object has the following default values for the readiness probe configuration.
    readinessProbe:
      # -- The HTTP GET request to use for the readiness probe.
      httpGet:
        path: /health
        port: http
      # -- The frequency in seconds to perform the readiness probe.
      periodSeconds: 10
      # -- The timeout in seconds for the readiness probe.
      timeoutSeconds: 5
      # -- The failure threshold for the readiness probe.
      failureThreshold: 3

    # -- Specifies autoscaling configurations for the deployment.
    autoscaling:
      # -- Whether to enable horizontal pod autoscaler.
      enabled: false
      # -- The minimum number of replicas for the deployment.
      minReplicas: 1
      # -- The maximum number of replicas for the deployment.
      maxReplicas: 100
      # -- The target CPU utilization percentage.
      targetCPUUtilizationPercentage: 80
      # targetMemoryUtilizationPercentage: 80

    # Environment variables to pass to containers. This is an object formatted like NAME: value or NAME: valueFrom: {object}
    env: {}

    nodeSelector: {}

    affinity: {}

    tolerations: []

  # @default -- This object has the following default values for the controller configuration.
  controller:
    # -- Service account configuration for the controller service.
    # @default -- This object has the following default values for the service account configuration.
    serviceAccount:
      # -- Specifies whether a service account should be created.
      create: true
      # -- Automatically mount a ServiceAccount's API credentials.
      automount: true
      # -- Annotations to add to the service account.
      annotations: {}
      # -- The name of the service account to use. If not set and create is true, a name is generated using the fullname template.
      name: ""

    # -- Additional arguments to pass to the Core Controller service
    extraArgs: []
      # - "--target=jobs-controller"

    # -- Annotations to add to the controller service deployment.
    annotations: {}

    # -- Annotations to add to the controller service pod.
    podAnnotations: {}

    # -- Labels for the controller service pod.
    podLabels: {}

    # -- Pod-level security context settings for the controller service.
    # @default -- This object has the following default values for the pod security context.
    podSecurityContext:
      # -- The file system group ID to use for all containers.
      fsGroup: 1000

    # -- Container-level security context settings for the controller service.
    securityContext: {}

    # -- Kubernetes deployment resources configuration for the controller service.
    # It is recommended to not specify default resources and to leave this as a conscious
    # choice. This also increases chances that the chart will run on environments with little
    # resources, such as minikube. If you want to specify resources, use the following
    # example, adjust the values as necessary, and remove the empty curly braces `{}`.
    # `limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi`
    resources: {}

    # -- Liveness probe configuration for the controller service.
    # @default -- This object has the following default values for the liveness probe configuration.
    livenessProbe:
      # -- The HTTP GET request to use for the readiness probe.
      httpGet:
        path: /health/live
        port: http
      # -- The frequency in seconds to perform the readiness probe.
      periodSeconds: 10
      # -- The timeout in seconds for the readiness probe.
      timeoutSeconds: 5
      # -- The failure threshold for the readiness probe.
      failureThreshold: 3

    # -- Readiness probe configuration for the controller service.
    # @default -- This object has the following default values for the readiness probe configuration.
    readinessProbe:
      # -- The HTTP GET request to use for the readiness probe.
      httpGet:
        path: /health/ready
        port: http
      # -- The frequency in seconds to perform the readiness probe.
      periodSeconds: 10
      # -- The timeout in seconds for the readiness probe.
      timeoutSeconds: 5
      # -- The failure threshold for the readiness probe.
      failureThreshold: 3

    # -- Additional environment variables to pass to containers. This is an object formatted like NAME: value or NAME: valueFrom: {object}.
    env: {}

    # -- Node selector configuration for the controller service.
    nodeSelector: {}

    # -- Affinity configuration for the controller service.
    affinity: {}

    tolerations: []

  logcollector:
    # -- Container image configuration for the log collector.
    # @default -- This object has the following default values for the image configuration.
    image:
      # -- The registry where the log collector image is located.
      repository: fluent/fluent-bit
      # -- The image tag to use.
      tag: 4.0.7
      # -- The image pull policy determining when to pull new images.
      pullPolicy: IfNotPresent

    # -- The command to run in the log collector container.
    command:
      ["/fluent-bit/bin/fluent-bit", "-c", "/fluent-bit/etc/fluent-bit.yaml"]

    # -- The number of replicas for the log collector.
    replicaCount: 1

    # -- Service account configuration for the log collector.
    # @default -- This object has the following default values for the service account configuration.
    serviceAccount:
      # -- Specifies whether a service account should be created.
      create: true
      # -- Automatically mount a ServiceAccount's API credentials.
      automount: true
      # -- Annotations to add to the service account.
      annotations: {}
      # -- The name of the service account to use. If not set and create is true, a name is generated using the fullname template.
      name: ""

    # -- Annotations to add to the log collector deployment.
    annotations: {}

    # -- Annotations to add to the log collector pods.
    podAnnotations: {}

    # -- Labels for the log collector pod.
    podLabels: {}

    # -- Pod-level security context settings for the log collector.
    # @default -- This object has the following default values for the pod security context.
    podSecurityContext:
      # -- The file system group ID to use for all containers.
      fsGroup: 1000

    # -- Container-level security context settings for the log collector.
    securityContext: {}

    # -- Service configuration for the log collector.
    # @default -- This object has the following default values for the service configuration.
    service:
      # -- The Kubernetes service type to create.
      type: ClusterIP
      # -- The port number to expose for the service.
      port: 4318

    # -- Kubernetes deployment resources configuration for the log collector.
    # It is recommended to not specify default resources and to leave this as a conscious
    # choice. This also increases chances that the chart will run on environments with little
    # resources, such as minikube. If you want to specify resources, use the following
    # example, adjust the values as necessary, and remove the empty curly braces `{}`.
    # `limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi`
    resources: {}

    # -- Liveness probe configuration for the log collector.
    # @default -- This object has the following default values for the liveness probe configuration.
    livenessProbe:
      # -- TCP socket configuration for liveness probe.
      tcpSocket:
        # -- Port for TCP socket liveness probe.
        port: 4318
      # -- Initial delay in seconds before liveness probe is initiated.
      initialDelaySeconds: 5
      # -- Period in seconds for liveness probe.
      periodSeconds: 10
      # -- Timeout in seconds for liveness probe.
      timeoutSeconds: 5
      # -- Failure threshold for liveness probe.
      failureThreshold: 3

    # -- Readiness probe configuration for the log collector.
    # @default -- This object has the following default values for the readiness probe configuration.
    readinessProbe:
      # -- HTTP GET request configuration for readiness probe.
      httpGet:
        # -- The HTTP path for the readiness probe.
        path: /api/v1/health
        # -- The port name for the readiness probe.
        port: 2020

      # -- Initial delay in seconds before readiness probe is initiated.
      initialDelaySeconds: 5
      # -- Period in seconds for readiness probe.
      periodSeconds: 5
      # -- Timeout in seconds for readiness probe.
      timeoutSeconds: 3
      # -- Failure threshold for readiness probe.
      failureThreshold: 3

    # -- Autoscaling configuration for the log collector.
    # @default -- This object has the following default values for the autoscaling configuration.
    autoscaling:
      # -- Whether to enable horizontal pod autoscaler.
      enabled: false
      # -- The minimum number of replicas for the deployment.
      minReplicas: 1
      # -- The maximum number of replicas for the deployment.
      maxReplicas: 1
      # -- The target CPU utilization percentage.
      targetCPUUtilizationPercentage: 80
      # -- The target memory utilization percentage.
      # targetMemoryUtilizationPercentage: 80

    # -- Additional environment variables to pass to containers. This is an object formatted like NAME: value or NAME: valueFrom: {object}.
    env: {}

    # -- Node selector configuration for the log collector.
    nodeSelector: {}

    # -- Affinity configuration for the log collector.
    affinity: {}

    tolerations: []

  logging:
    # -- Sidecar configuration for the log collector sidecar.
    # @default -- This object has the following default values for the sidecar configuration.
    sidecar:
      # -- Whether to enable the logging sidecar. Set to false to disable logging sidecars being attached to jobs.
      enabled: true
      # -- Container image configuration for the log collector sidecar.
      # @default -- This object has the following default values for the image configuration.
      image:
        # -- The registry where the log collector sidecar image is located.
        repository: fluent/fluent-bit
        # -- The image tag to use.
        tag: 4.0.7

  # -- PostgreSQL configuration for the NeMo Jobs microservice.
  # @default -- This object has the following default values for the PostgreSQL configuration.
  postgresql:
    # -- Whether to install the default PostgreSQL Helm chart. If enabled, the NeMo Jobs microservice Helm chart uses the [PostgreSQL Helm chart from Bitnami](https://github.com/bitnami/charts/blob/main/bitnami/postgresql/values.yaml) to create a PostgreSQL database.
    enabled: true
    # -- The name override for the PostgreSQL database.
    nameOverride: jobsdb
    image:
      repository: bitnamilegacy/postgresql
    volumePermissions:
      image:
        repository: bitnamilegacy/os-shell
    metrics:
      image:
        repository: bitnamilegacy/postgres-exporter
    # -- Service account configuration for PostgreSQL.
    # @default -- This object has the following default values for the service account configuration.
    serviceAccount:
      # -- The service account name for PostgreSQL.
      name: jobs-postgresql
      # -- Specifies whether to create a new service account for PostgreSQL.
      create: true
    # -- Authentication configuration for PostgreSQL.
    # @default -- This object has the following default values for the authentication configuration.
    auth:
      # -- Whether to assign a password to the "postgres" admin user. If disabled, remote access is blocked for this user.
      enablePostgresUser: true
      # -- The user name to use for the PostgreSQL database.
      username: nemo
      # -- The password for the PostgreSQL user.
      password: nemo
      # -- The name for a custom database to create.
      database: jobs
      # -- The name of an existing secret to use for PostgreSQL credentials.
      existingSecret: ""
    # -- The PostgreSQL architecture. Available options are `standalone` or `replication`.
    architecture: standalone
    # -- Service configuration for PostgreSQL.
    # @default -- This object has the following default values for the service configuration.
    service:
      # -- Port configuration for PostgreSQL service.
      ports:
        # -- Port number for PostgreSQL service.
        postgresql: 5432
    # -- Persistence configuration for PostgreSQL.
    # @default -- This object has the following default values for the persistence configuration.
    persistence:
      # -- Whether to enable persistent volume.
      enabled: true
      # -- Size of the persistent volume.
      size: 10Gi

  # -- External PostgreSQL configuration settings. These values are only used when postgresql.enabled is set to false.
  # @default -- This object has the following default values for the external PostgreSQL configuration.
  externalDatabase:
    # -- External database host address.
    host: localhost
    # -- External database port number.
    port: 5432
    # -- Database username for Jobs service.
    user: nemo
    # -- Jobs database name.
    database: jobs
    # -- Name of an existing secret resource containing the database credentials.
    existingSecret: ""
    # -- Name of an existing secret key containing the database credentials.
    existingSecretPasswordKey: ""
    # -- URI secret configuration for external database.
    # @default -- This object has the following default values for the URI secret configuration.
    uriSecret:
      # -- Name of the URI secret.
      name: ""
      # -- Key in the URI secret containing the database URI.
      key: ""

  databaseMigrations:
    # -- Enable or disable database migrations job
    enabled: true

    # -- Additional arguments to pass to the database migration jobs
    extraArgs: []
      # - "--target=jobs-api"

    restartPolicy: OnFailure

    # -- Annotations to add to the database migration job.
    annotations: {}

    # -- Annotations to add to the database migration job pods.
    podAnnotations: {}

    # -- Labels for the database migration job pods.
    podLabels: {}

    podSecurityContext:
      fsGroup: 1000

    securityContext: {}

    resources: {}

    nodeSelector: {}

    affinity: {}

    tolerations: []

auditor:
  # -- Specifies whether to install the NeMo Auditor microservice.
  serviceName: nemo-auditor

  # -- Number of replicas for the Auditor deployment.
  replicaCount: 1

  # -- Container image configuration.
  image:
    repository: nvcr.io/nvidia/nemo-microservices/auditor
    pullPolicy: IfNotPresent
    tag: ""

  # -- Service account configuration.
  serviceAccount:
    # -- Specifies whether a service account should be created.
    create: true
    # -- Automatically mount a ServiceAccount's API credentials.
    automount: true
    # -- Annotations to add to the service account.
    annotations: {}
    # -- The name of the service account to use. If not set and create is true, a name is generated using the fullname template.
    name: ""

  # -- Annotations to add to pods.
  podAnnotations: {}
  # -- Labels to add to pods.
  podLabels: {}

  # -- Security context for the pod.
  podSecurityContext:
    fsGroup: 1000

  # -- Security context for the container.
  securityContext: {}

  # -- Service configuration.
  service:
    type: ClusterIP
    port: 5000

  # -- Resource requests and limits.
  resources: {}

  # -- Liveness probe configuration.
  livenessProbe:
    httpGet:
      path: /health/live
      port: http
    initialDelaySeconds: 0
    periodSeconds: 10
    timeoutSeconds: 600
    failureThreshold: 100

  # -- Readiness probe configuration.
  readinessProbe:
    httpGet:
      path: /health/ready
      port: http
    initialDelaySeconds: 0
    periodSeconds: 10
    timeoutSeconds: 600
    failureThreshold: 100

  # -- Autoscaling configuration.
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 1
    targetCPUUtilizationPercentage: 80

  # -- Environment variables to pass to containers. This is an object formatted like NAME: value or NAME: valueFrom: {object}
  env: {}

  # -- Node selector for pod assignment.
  nodeSelector: {}

  # -- Affinity rules for pod assignment.
  affinity: {}

  # -- Tolerations for pod assignment.
  tolerations: []

  # -- Additional volumes on the output Deployment definition.
  volumes: []

  # -- Additional volume mounts on the output Deployment definition.
  volumeMounts: []

  # -- Name of secret which holds mapping from api key names to the keys themselves
  auditorApiKeysSecretName: "auditor-api-keys"

  # -- PostgreSQL configuration for the NeMo Auditor microservice.
  postgresql:
    # -- Whether to install the default PostgreSQL Helm chart. If enabled, the NeMo Auditor microservice Helm chart uses the [PostgreSQL Helm chart from Bitnami](https://github.com/bitnami/charts/blob/main/bitnami/postgresql/values.yaml) to create a PostgreSQL database.
    enabled: true
    image:
      repository: bitnamilegacy/postgresql
    volumePermissions:
      image:
        repository: bitnamilegacy/os-shell
    metrics:
      image:
        repository: bitnamilegacy/postgres-exporter
    # -- The name override for the Auditor PostgreSQL database.
    nameOverride: auditdb
    # -- The architecture for the default PostgreSQL service.
    architecture: standalone
    serviceAccount:
      # -- The name of the service account for PostgreSQL.
      name: auditor-postgresql
      # -- Specifies whether to create a new service account for PostgreSQL.
      create: true
    auth:
      # -- Whether to assign a password to the "postgres" admin user. If disabled, remote access is blocked for this user.
      enablePostgresUser: true
      # -- The user name to use for the PostgreSQL database.
      username: nemo
      # -- The password for the PostgreSQL user.
      password: nemo
      # -- The name for a custom database to create.
      database: auditor
      # -- The name of an existing secret to use for PostgreSQL credentials.
      existingSecret: ""
    service:
      ports:
        postgresql: 5432
    persistence:
      enabled: true
      size: 10Gi

  # -- External PostgreSQL configuration.
  externalDatabase:
    # -- The database host.
    host: ""
    # -- The database port.
    port: 5432
    # -- The database user.
    user: nemo
    # -- The database name.
    database: auditor
    # -- The name of an existing secret to use for PostgreSQL credentials.
    existingSecret: ""
    # -- The name of an existing secret key to use for PostgreSQL credentials.
    existingSecretPasswordKey: ""

safe-synthesizer:
  replicaCount: 1

  image:
    repository: nvcr.io/nvidia/nemo-microservices/safe-synthesizer-api
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: ""

  jobsImage:
    repository: nvcr.io/nvidia/nemo-microservices/safe-synthesizer
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: ""

  config:
    classify_llm_endpoint_url: https://integrate.api.nvidia.com/v1
    classify_llm_model_id: qwen/qwen2.5-coder-32b-instruct

  classify:
    apiKey: ""

  imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: ""
  serviceName: nemo-safe-synthesizer

  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Automatically mount a ServiceAccount's API credentials?
    automount: true
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""

  podAnnotations: {}
  podLabels: {}

  podSecurityContext:
    fsGroup: 1000

  securityContext:
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  service:
    type: ClusterIP
    port: 8000

  resources:
    {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 500m
    #   memory: 1Gi
    # requests:
    #   cpu: 250m
    #   memory: 512Mi

  livenessProbe:
    httpGet:
      path: /health
      port: http
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3

  readinessProbe:
    httpGet:
      path: /health
      port: http
    initialDelaySeconds: 5
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 3

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  # Environment variables to pass to containers. This is an object formatted like NAME: value or NAME: valueFrom: {object}
  env:
    # Safe Synthesizer service specific environment variables
    LOG_LEVEL: "INFO"
    NEMO_SAFE_SYNTHESIZER_PORT: 8000
    HOST_IP:
      valueFrom:
        fieldRef:
          fieldPath: status.hostIP

  # Additional volumes on the output Deployment definition.
  volumes: []
  # Example:
  # - name: safe-synthesizer-data
  #   persistentVolumeClaim:
  #     claimName: safe-synthesizer-data-pvc

  # Additional volume mounts on the output Deployment definition.
  volumeMounts: []
  # Example:
  # - name: safe-synthesizer-data
  #   mountPath: /app/data

  nodeSelector: {}

  tolerations: []

  affinity: {}

intake:
  replicaCount: 1

  image:
    repository: nvcr.io/nvidia/nemo-microservices/intake
    pullPolicy: IfNotPresent
    tag: ""

  postgresql:
    enabled: true
    nameOverride: intakedb
    image:
      repository: bitnamilegacy/postgresql
    volumePermissions:
      image:
        repository: bitnamilegacy/os-shell
    metrics:
      image:
        repository: bitnamilegacy/postgres-exporter
    auth:
      username: user
      password: password
      postgresPassword: password
    primary:
      passwordUpdateJob:
        enabled: true

      persistence:
        enabled: true
        size: 50Gi
        #accessModes: ReadWriteMany
        #storageClass: "local-nfs"

  imagePullSecrets:
    - name: gitlab-imagepull
  nameOverride: ""
  fullnameOverride: ""

  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Automatically mount a ServiceAccount's API credentials?
    automount: true
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""

  # podAnnotations:
  #   # Add annotation to use NVIDIA runtime
  #   nvidia.com/gpu: "true"

  podLabels: {}

  podSecurityContext:
    fsGroup: 1000

  securityContext:
    {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  service:
    type: ClusterIP
    port: 8000

  resources:
    {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  livenessProbe:
    httpGet:
      path: /health
      port: http
    initialDelaySeconds: 0
    periodSeconds: 10
    timeoutSeconds: 600
    failureThreshold: 100

  readinessProbe:
    httpGet:
      path: /health
      port: http
    initialDelaySeconds: 0
    periodSeconds: 10
    timeoutSeconds: 600
    failureThreshold: 100

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 1
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  # Additional volumes on the output Deployment definition.
  # volumes:
  #   - name: nemo-intake-volume
  #     persistentVolumeClaim:
  #       claimName: nemo-intake-pvc

  # volumeMounts:
  #   - name: nemo-intake-volume
  #     mountPath: /app/backend/data

  # persistence:
  #   enabled: true
  #   size: 50Gi
  #   accessModes: ReadWriteMany
  #   storageClass: "local-nfs"

  # Environment variables to pass to containers. This is an object formatted like NAME: value or NAME: valueFrom: {object}
  env:
    POSTGRES_URI: "postgresql://postgres:password@nemo-intakedb:5432/postgres"
    NIM_URI: "http://nemo-nim-proxy:8000/v1"
    REDACT_COMPLETIONS: "true"

  nodeSelector: {}

  affinity: {}

  serviceName: nemo-intake
